[
  {
    "objectID": "content/01_journal/AutomatedML.html",
    "href": "content/01_journal/AutomatedML.html",
    "title": "Automated Machine Learning with H2O II",
    "section": "",
    "text": "1 Loading the training & test dataset\n\nlibrary(h2o)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(recipes)\nlibrary(parsnip)\nlibrary(yardstick)\n\n#> \n#> Attaching package: 'yardstick'\n\n\n#> The following object is masked from 'package:readr':\n#> \n#>     spec\n\nlibrary(tidymodels)\n\n#> ── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n\n\n#> ✔ broom        1.0.4     ✔ rsample      1.1.1\n#> ✔ dials        1.2.0     ✔ tune         1.1.1\n#> ✔ infer        1.0.4     ✔ workflows    1.1.3\n#> ✔ modeldata    1.1.0     ✔ workflowsets 1.0.1\n\n\n#> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n#> ✖ scales::discard() masks purrr::discard()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ recipes::fixed()  masks stringr::fixed()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ yardstick::spec() masks readr::spec()\n#> ✖ recipes::step()   masks stats::step()\n#> • Use tidymodels_prefer() to resolve common conflicts.\n\n\n\nproduct_backorders_tbl <- read_xlsx(\"./../../assets/DataSets/product_backorders.xlsx\")\nset.seed(seed = 1113)\nsplit_obj <- rsample::initial_split(product_backorders_tbl, prop = 0.85)\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl <- testing(split_obj)\n\n\nrecipe_obj <- recipe(went_on_backorder ~., data = train_readable_tbl) %>% \n  step_zv(all_predictors()) %>% \n  step_mutate_at(in_transit_qty, local_bo_qty, fn = as.factor) %>% \n  prep()\n\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\n\n2 Intializing H2O\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         18 minutes 5 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 6 days \n#>     H2O cluster name:           H2O_started_from_R_Mohamed_yyy113 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.38 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\n\n\n3 Spliting data into a training and a validation data frame\n\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n4 Specifing the response and predictor variables\n\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o), y)\n\n\n5 Running AutoML specifying the stopping criterion\n\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 30,\n  nfolds            = 5 \n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==                                                                    |   3%\n#> 17:18:13.778: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 17:18:13.780: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\n6 Viewing the leaderboard\n\ntypeof(automl_models_h2o)\n\n#> [1] \"S4\"\n\nslotNames(automl_models_h2o)\n\n#> [1] \"project_name\"   \"leader\"         \"leaderboard\"    \"event_log\"     \n#> [5] \"modeling_steps\" \"training_info\"\n\nautoml_models_h2o@leaderboard\n\n#>                                                  model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_1_AutoML_2_20230603_171813 0.9129121 0.2162742\n#> 2 StackedEnsemble_BestOfFamily_2_AutoML_2_20230603_171813 0.9118148 0.2181967\n#> 3                          GBM_1_AutoML_2_20230603_171813 0.9113482 0.2183839\n#> 4 StackedEnsemble_BestOfFamily_1_AutoML_2_20230603_171813 0.9113135 0.2163088\n#> 5                          GBM_2_AutoML_2_20230603_171813 0.9082019 0.2266387\n#> 6                          GBM_3_AutoML_2_20230603_171813 0.8978494 0.2343274\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.6596054            0.1928838 0.2497972 0.06239865\n#> 2 0.6570563            0.1953317 0.2508606 0.06293106\n#> 3 0.6524506            0.2039151 0.2529613 0.06398943\n#> 4 0.6524743            0.2039151 0.2519839 0.06349587\n#> 5 0.6361293            0.1665823 0.2579181 0.06652176\n#> 6 0.6262939            0.2031958 0.2621152 0.06870437\n#> \n#> [10 rows x 7 columns]\n\nautoml_models_h2o@leader\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_1_AutoML_2_20230603_171813 \n#> Model Summary for Stacked Ensemble: \n#>                                     key            value\n#> 1                     Stacking strategy cross_validation\n#> 2  Number of base models (used / total)              4/6\n#> 3      # GBM base models (used / total)              3/4\n#> 4      # DRF base models (used / total)              1/1\n#> 5      # GLM base models (used / total)              0/1\n#> 6                 Metalearner algorithm              GLM\n#> 7    Metalearner fold assignment scheme           Random\n#> 8                    Metalearner nfolds                5\n#> 9               Metalearner fold_column               NA\n#> 10   Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.02256752\n#> RMSE:  0.1502249\n#> LogLoss:  0.0928187\n#> Mean Per-Class Error:  0.05861237\n#> AUC:  0.9939095\n#> AUCPR:  0.9622149\n#> Gini:  0.987819\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     8723  106 0.012006   =106/8829\n#> Yes     125 1063 0.105219   =125/1188\n#> Totals 8848 1169 0.023061  =231/10017\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.443874    0.901994 167\n#> 2                       max f2  0.290264    0.916734 207\n#> 3                 max f0point5  0.586121    0.921875 131\n#> 4                 max accuracy  0.443874    0.976939 167\n#> 5                max precision  0.999895    1.000000   0\n#> 6                   max recall  0.055674    1.000000 316\n#> 7              max specificity  0.999895    1.000000   0\n#> 8             max absolute_mcc  0.443874    0.888964 167\n#> 9   max min_per_class_accuracy  0.262725    0.960438 215\n#> 10 max mean_per_class_accuracy  0.253941    0.960722 218\n#> 11                     max tns  0.999895 8829.000000   0\n#> 12                     max fns  0.999895 1183.000000   0\n#> 13                     max fps  0.000141 8829.000000 399\n#> 14                     max tps  0.055674 1188.000000 316\n#> 15                     max tnr  0.999895    1.000000   0\n#> 16                     max fnr  0.999895    0.995791   0\n#> 17                     max fpr  0.000141    1.000000 399\n#> 18                     max tpr  0.055674    1.000000 316\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.05636438\n#> RMSE:  0.2374118\n#> LogLoss:  0.1939746\n#> Mean Per-Class Error:  0.1600841\n#> AUC:  0.932599\n#> AUCPR:  0.692712\n#> Gini:  0.8651981\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     1986 118 0.056084  =118/2104\n#> Yes      75 209 0.264085    =75/284\n#> Totals 2061 327 0.080821  =193/2388\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.307302    0.684124 182\n#> 2                       max f2  0.179628    0.743113 228\n#> 3                 max f0point5  0.568220    0.709325 108\n#> 4                 max accuracy  0.568220    0.925042 108\n#> 5                max precision  0.984220    1.000000   0\n#> 6                   max recall  0.001627    1.000000 395\n#> 7              max specificity  0.984220    1.000000   0\n#> 8             max absolute_mcc  0.307302    0.640134 182\n#> 9   max min_per_class_accuracy  0.120199    0.860741 260\n#> 10 max mean_per_class_accuracy  0.094704    0.868058 280\n#> 11                     max tns  0.984220 2104.000000   0\n#> 12                     max fns  0.984220  282.000000   0\n#> 13                     max fps  0.000080 2104.000000 399\n#> 14                     max tps  0.001627  284.000000 395\n#> 15                     max tnr  0.984220    1.000000   0\n#> 16                     max fnr  0.984220    0.992958   0\n#> 17                     max fpr  0.000080    1.000000 399\n#> 18                     max tpr  0.001627    1.000000 395\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.05990554\n#> RMSE:  0.2447561\n#> LogLoss:  0.2022788\n#> Mean Per-Class Error:  0.1882843\n#> AUC:  0.9276281\n#> AUCPR:  0.6674549\n#> Gini:  0.8552561\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     11472  697 0.057277   =697/12169\n#> Yes      523 1115 0.319292    =523/1638\n#> Totals 11995 1812 0.088361  =1220/13807\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.306886     0.646377 202\n#> 2                       max f2  0.171424     0.717570 255\n#> 3                 max f0point5  0.521759     0.675008 133\n#> 4                 max accuracy  0.521759     0.919823 133\n#> 5                max precision  0.999941     1.000000   0\n#> 6                   max recall  0.000093     1.000000 399\n#> 7              max specificity  0.999941     1.000000   0\n#> 8             max absolute_mcc  0.306886     0.597027 202\n#> 9   max min_per_class_accuracy  0.116405     0.851648 286\n#> 10 max mean_per_class_accuracy  0.118888     0.851795 284\n#> 11                     max tns  0.999941 12169.000000   0\n#> 12                     max fns  0.999941  1636.000000   0\n#> 13                     max fps  0.000093 12169.000000 399\n#> 14                     max tps  0.000093  1638.000000 399\n#> 15                     max tnr  0.999941     1.000000   0\n#> 16                     max fnr  0.999941     0.998779   0\n#> 17                     max fpr  0.000093     1.000000 399\n#> 18                     max tpr  0.000093     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.912110  0.010254   0.924431   0.896320   0.915193   0.914527\n#> auc         0.927906  0.006218   0.930508   0.921310   0.936010   0.921961\n#> err         0.087890  0.010254   0.075569   0.103680   0.084807   0.085473\n#> err_count 243.000000 31.851217 206.000000 293.000000 235.000000 233.000000\n#> f0point5    0.631683  0.037241   0.664911   0.568570   0.640703   0.651090\n#>           cv_5_valid\n#> accuracy    0.910080\n#> auc         0.929740\n#> err         0.089920\n#> err_count 248.000000\n#> f0point5    0.633142\n#> \n#> ---\n#>                          mean        sd  cv_1_valid  cv_2_valid  cv_3_valid\n#> precision            0.619081  0.049294    0.666667    0.541860    0.614458\n#> r2                   0.427296  0.032053    0.443608    0.377120    0.457528\n#> recall               0.696671  0.056097    0.657980    0.708207    0.772727\n#> residual_deviance 1116.371300 62.933964 1039.493400 1194.839500 1068.809300\n#> rmse                 0.244590  0.007440    0.235804    0.253126    0.238557\n#> specificity          0.941084  0.015262    0.958247    0.921105    0.934453\n#>                    cv_4_valid  cv_5_valid\n#> precision            0.657233    0.615190\n#> r2                   0.414876    0.443345\n#> recall               0.627628    0.716814\n#> residual_deviance 1154.463900 1124.250600\n#> rmse                 0.250491    0.244972\n#> specificity          0.954450    0.937164\n\n\n\n7 Predicting using Leader Model\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         6 minutes 20 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 6 days \n#>     H2O cluster name:           H2O_started_from_R_Mohamed_ewp216 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.15 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\nstacked_ensemble_h2o <- h2o.loadModel(\"./../../assets/Models/GBM_1_AutoML_6_20230602_132940\")\n\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         6 minutes 20 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 6 days \n#>     H2O cluster name:           H2O_started_from_R_Mohamed_ewp216 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.15 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\npredictions <- h2o.predict(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(predictions)\n\n#> [1] \"environment\"\n\npredictions_tbl <- predictions %>% as_tibble()\n\npredictions_tbl"
  },
  {
    "objectID": "content/01_journal/AutomatedML_I.html",
    "href": "content/01_journal/AutomatedML_I.html",
    "title": "Automated Machine Learning with H2O I",
    "section": "",
    "text": "# Libraries \nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(GGally)\n\n# Load Data data definitions\nemployee_attrition_tbl <- read_excel(\"./../../assets/DataSets/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.xlsx\")\npath_data_definitions <- \"./../../assets/DataSets/data_definitions.xlsx\"\ndefinitions_raw_tbl   <- read_excel(path_data_definitions, sheet = 1, col_names = FALSE)\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\n# Descriptive Features\nemployee_attrition_tbl %>% select(Age, DistanceFromHome, Gender, MaritalStatus, NumCompaniesWorked, Over18)\n\n\n\n  \n\n\n# Employment Features\nemployee_attrition_tbl %>% select(Department, EmployeeCount, EmployeeNumber, JobInvolvement, JobLevel, JobRole, JobSatisfaction)\n\n\n\n  \n\n\n# Compensation Features\nemployee_attrition_tbl %>% select(DailyRate, HourlyRate, MonthlyIncome, MonthlyRate, PercentSalaryHike, StockOptionLevel)\n\n\n\n  \n\n\n# Survery Results\nemployee_attrition_tbl %>% select(EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance)\n\n\n\n  \n\n\n# Performance Data\nemployee_attrition_tbl %>% select(JobInvolvement, PerformanceRating)\n\n\n\n  \n\n\n# Work-Life Features\nemployee_attrition_tbl %>% select(BusinessTravel, OverTime)\n\n\n\n  \n\n\n# Training & Education\nemployee_attrition_tbl %>% select(Education, EducationField, TrainingTimesLastYear)\n\n\n\n  \n\n\n# Time-Based Features\nemployee_attrition_tbl %>% select(TotalWorkingYears, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager)\n\n\n\n  \n\n\n# Step 1: Data Summarization -----\n\nskim(employee_attrition_tbl)\n\n\nData summary\n\n\nName\nemployee_attrition_tbl\n\n\nNumber of rows\n1470\n\n\nNumber of columns\n35\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n9\n\n\nnumeric\n26\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAttrition\n0\n1\n2\n3\n0\n2\n0\n\n\nBusinessTravel\n0\n1\n10\n17\n0\n3\n0\n\n\nDepartment\n0\n1\n5\n22\n0\n3\n0\n\n\nEducationField\n0\n1\n5\n16\n0\n6\n0\n\n\nGender\n0\n1\n4\n6\n0\n2\n0\n\n\nJobRole\n0\n1\n7\n25\n0\n9\n0\n\n\nMaritalStatus\n0\n1\n6\n8\n0\n3\n0\n\n\nOver18\n0\n1\n1\n1\n0\n1\n0\n\n\nOverTime\n0\n1\n2\n3\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nAge\n0\n1\n36.92\n9.14\n18\n30.00\n36.0\n43.00\n60\n▂▇▇▃▂\n\n\nDailyRate\n0\n1\n802.49\n403.51\n102\n465.00\n802.0\n1157.00\n1499\n▇▇▇▇▇\n\n\nDistanceFromHome\n0\n1\n9.19\n8.11\n1\n2.00\n7.0\n14.00\n29\n▇▅▂▂▂\n\n\nEducation\n0\n1\n2.91\n1.02\n1\n2.00\n3.0\n4.00\n5\n▂▃▇▆▁\n\n\nEmployeeCount\n0\n1\n1.00\n0.00\n1\n1.00\n1.0\n1.00\n1\n▁▁▇▁▁\n\n\nEmployeeNumber\n0\n1\n1024.87\n602.02\n1\n491.25\n1020.5\n1555.75\n2068\n▇▇▇▇▇\n\n\nEnvironmentSatisfaction\n0\n1\n2.72\n1.09\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nHourlyRate\n0\n1\n65.89\n20.33\n30\n48.00\n66.0\n83.75\n100\n▇▇▇▇▇\n\n\nJobInvolvement\n0\n1\n2.73\n0.71\n1\n2.00\n3.0\n3.00\n4\n▁▃▁▇▁\n\n\nJobLevel\n0\n1\n2.06\n1.11\n1\n1.00\n2.0\n3.00\n5\n▇▇▃▂▁\n\n\nJobSatisfaction\n0\n1\n2.73\n1.10\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nMonthlyIncome\n0\n1\n6502.93\n4707.96\n1009\n2911.00\n4919.0\n8379.00\n19999\n▇▅▂▁▂\n\n\nMonthlyRate\n0\n1\n14313.10\n7117.79\n2094\n8047.00\n14235.5\n20461.50\n26999\n▇▇▇▇▇\n\n\nNumCompaniesWorked\n0\n1\n2.69\n2.50\n0\n1.00\n2.0\n4.00\n9\n▇▃▂▂▁\n\n\nPercentSalaryHike\n0\n1\n15.21\n3.66\n11\n12.00\n14.0\n18.00\n25\n▇▅▃▂▁\n\n\nPerformanceRating\n0\n1\n3.15\n0.36\n3\n3.00\n3.0\n3.00\n4\n▇▁▁▁▂\n\n\nRelationshipSatisfaction\n0\n1\n2.71\n1.08\n1\n2.00\n3.0\n4.00\n4\n▅▅▁▇▇\n\n\nStandardHours\n0\n1\n80.00\n0.00\n80\n80.00\n80.0\n80.00\n80\n▁▁▇▁▁\n\n\nStockOptionLevel\n0\n1\n0.79\n0.85\n0\n0.00\n1.0\n1.00\n3\n▇▇▁▂▁\n\n\nTotalWorkingYears\n0\n1\n11.28\n7.78\n0\n6.00\n10.0\n15.00\n40\n▇▇▂▁▁\n\n\nTrainingTimesLastYear\n0\n1\n2.80\n1.29\n0\n2.00\n3.0\n3.00\n6\n▂▇▇▂▃\n\n\nWorkLifeBalance\n0\n1\n2.76\n0.71\n1\n2.00\n3.0\n3.00\n4\n▁▃▁▇▂\n\n\nYearsAtCompany\n0\n1\n7.01\n6.13\n0\n3.00\n5.0\n9.00\n40\n▇▂▁▁▁\n\n\nYearsInCurrentRole\n0\n1\n4.23\n3.62\n0\n2.00\n3.0\n7.00\n18\n▇▃▂▁▁\n\n\nYearsSinceLastPromotion\n0\n1\n2.19\n3.22\n0\n0.00\n1.0\n3.00\n15\n▇▁▁▁▁\n\n\nYearsWithCurrManager\n0\n1\n4.12\n3.57\n0\n2.00\n3.0\n7.00\n17\n▇▂▅▁▁\n\n\n\n\n# Character Data Type\nemployee_attrition_tbl %>%\n  select_if(is.character) %>%\n  glimpse()\n\n#> Rows: 1,470\n#> Columns: 9\n#> $ Attrition      <chr> \"Yes\", \"No\", \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n#> $ BusinessTravel <chr> \"Travel_Rarely\", \"Travel_Frequently\", \"Travel_Rarely\", …\n#> $ Department     <chr> \"Sales\", \"Research & Development\", \"Research & Developm…\n#> $ EducationField <chr> \"Life Sciences\", \"Life Sciences\", \"Other\", \"Life Scienc…\n#> $ Gender         <chr> \"Female\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fe…\n#> $ JobRole        <chr> \"Sales Executive\", \"Research Scientist\", \"Laboratory Te…\n#> $ MaritalStatus  <chr> \"Single\", \"Married\", \"Single\", \"Married\", \"Married\", \"S…\n#> $ Over18         <chr> \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", …\n#> $ OverTime       <chr> \"Yes\", \"No\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\", \"No\", \"No…\n\n# Get \"levels\"\nemployee_attrition_tbl %>%\n  select_if(is.character) %>%\n  map(unique)\n\n#> $Attrition\n#> [1] \"Yes\" \"No\" \n#> \n#> $BusinessTravel\n#> [1] \"Travel_Rarely\"     \"Travel_Frequently\" \"Non-Travel\"       \n#> \n#> $Department\n#> [1] \"Sales\"                  \"Research & Development\" \"Human Resources\"       \n#> \n#> $EducationField\n#> [1] \"Life Sciences\"    \"Other\"            \"Medical\"          \"Marketing\"       \n#> [5] \"Technical Degree\" \"Human Resources\" \n#> \n#> $Gender\n#> [1] \"Female\" \"Male\"  \n#> \n#> $JobRole\n#> [1] \"Sales Executive\"           \"Research Scientist\"       \n#> [3] \"Laboratory Technician\"     \"Manufacturing Director\"   \n#> [5] \"Healthcare Representative\" \"Manager\"                  \n#> [7] \"Sales Representative\"      \"Research Director\"        \n#> [9] \"Human Resources\"          \n#> \n#> $MaritalStatus\n#> [1] \"Single\"   \"Married\"  \"Divorced\"\n#> \n#> $Over18\n#> [1] \"Y\"\n#> \n#> $OverTime\n#> [1] \"Yes\" \"No\"\n\n# Proportions    \nemployee_attrition_tbl %>%\n  select_if(is.character) %>%\n  map(~ table(.) %>% prop.table())\n\n#> $Attrition\n#> .\n#>        No       Yes \n#> 0.8387755 0.1612245 \n#> \n#> $BusinessTravel\n#> .\n#>        Non-Travel Travel_Frequently     Travel_Rarely \n#>         0.1020408         0.1884354         0.7095238 \n#> \n#> $Department\n#> .\n#>        Human Resources Research & Development                  Sales \n#>             0.04285714             0.65374150             0.30340136 \n#> \n#> $EducationField\n#> .\n#>  Human Resources    Life Sciences        Marketing          Medical \n#>       0.01836735       0.41224490       0.10816327       0.31564626 \n#>            Other Technical Degree \n#>       0.05578231       0.08979592 \n#> \n#> $Gender\n#> .\n#> Female   Male \n#>    0.4    0.6 \n#> \n#> $JobRole\n#> .\n#> Healthcare Representative           Human Resources     Laboratory Technician \n#>                0.08911565                0.03537415                0.17619048 \n#>                   Manager    Manufacturing Director         Research Director \n#>                0.06938776                0.09863946                0.05442177 \n#>        Research Scientist           Sales Executive      Sales Representative \n#>                0.19863946                0.22176871                0.05646259 \n#> \n#> $MaritalStatus\n#> .\n#>  Divorced   Married    Single \n#> 0.2224490 0.4578231 0.3197279 \n#> \n#> $Over18\n#> .\n#> Y \n#> 1 \n#> \n#> $OverTime\n#> .\n#>        No       Yes \n#> 0.7170068 0.2829932\n\n# Numeric Data\nemployee_attrition_tbl %>%\n  select_if(is.numeric) %>%\n  map(~ unique(.) %>% length())\n\n#> $Age\n#> [1] 43\n#> \n#> $DailyRate\n#> [1] 886\n#> \n#> $DistanceFromHome\n#> [1] 29\n#> \n#> $Education\n#> [1] 5\n#> \n#> $EmployeeCount\n#> [1] 1\n#> \n#> $EmployeeNumber\n#> [1] 1470\n#> \n#> $EnvironmentSatisfaction\n#> [1] 4\n#> \n#> $HourlyRate\n#> [1] 71\n#> \n#> $JobInvolvement\n#> [1] 4\n#> \n#> $JobLevel\n#> [1] 5\n#> \n#> $JobSatisfaction\n#> [1] 4\n#> \n#> $MonthlyIncome\n#> [1] 1349\n#> \n#> $MonthlyRate\n#> [1] 1427\n#> \n#> $NumCompaniesWorked\n#> [1] 10\n#> \n#> $PercentSalaryHike\n#> [1] 15\n#> \n#> $PerformanceRating\n#> [1] 2\n#> \n#> $RelationshipSatisfaction\n#> [1] 4\n#> \n#> $StandardHours\n#> [1] 1\n#> \n#> $StockOptionLevel\n#> [1] 4\n#> \n#> $TotalWorkingYears\n#> [1] 40\n#> \n#> $TrainingTimesLastYear\n#> [1] 7\n#> \n#> $WorkLifeBalance\n#> [1] 4\n#> \n#> $YearsAtCompany\n#> [1] 37\n#> \n#> $YearsInCurrentRole\n#> [1] 19\n#> \n#> $YearsSinceLastPromotion\n#> [1] 16\n#> \n#> $YearsWithCurrManager\n#> [1] 18\n\nemployee_attrition_tbl %>%\n  select_if(is.numeric) %>%\n  map_df(~ unique(.) %>% length()) %>%\n  # Select all columns\n  pivot_longer(everything()) %>%\n  arrange(value) %>%\n  filter(value <= 10)\n\n\n\n  \n\n\nemployee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n  ggpairs() \n\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nemployee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n  ggpairs(aes(color = Attrition), lower = \"blank\", legend = 1,\n          diag  = list(continuous = wrap(\"densityDiag\", alpha = 0.5))) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n# Create data tibble, to potentially debug the plot_ggpairs function (because it has a data argument)\ndata <- employee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome)\n\nplot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {\n  \n  color_expr <- enquo(color)\n  \n  if (rlang::quo_is_null(color_expr)) {\n    \n    g <- data %>%\n      ggpairs(lower = \"blank\") \n    \n  } else {\n    \n    color_name <- quo_name(color_expr)\n    \n    g <- data %>%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")\n  }\n  \n  return(g)\n  \n}"
  },
  {
    "objectID": "content/01_journal/AutomatedML_I.html#compensation-features",
    "href": "content/01_journal/AutomatedML_I.html#compensation-features",
    "title": "Automated Machine Learning with H2O I",
    "section": "\n2.1 Compensation Features",
    "text": "2.1 Compensation Features\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %>%\n  plot_ggpairs(Attrition)\n\n#> Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#> ℹ Please use tidy evaluation idioms with `aes()`.\n#> ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Monthly Income and Attrition?\n\nAnswer: That those are staying have a lower Monthly Income.\n\nWhat can you deduce about the interaction between Percent Salary Hike and Attrition?\n\nAnswer: It’s difficult to deduce anything based on the visualization\n\nWhat can you deduce about the interaction between Stock Option Level and Attrition?\n\nAnswer: Those that are leaving the company have a higher stock option level"
  },
  {
    "objectID": "content/01_journal/AutomatedML_I.html#survey-results-satisfaction-level-worklifebalance",
    "href": "content/01_journal/AutomatedML_I.html#survey-results-satisfaction-level-worklifebalance",
    "title": "Automated Machine Learning with H2O I",
    "section": "\n2.2 Survey Results: Satisfaction level, WorkLifeBalance",
    "text": "2.2 Survey Results: Satisfaction level, WorkLifeBalance\n\nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %>%\n    plot_ggpairs(Attrition)\n\n#> Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#> ℹ Please use tidy evaluation idioms with `aes()`.\n#> ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Environment Satisfaction and Attrition?\n\nAnswer: A higher proportion of those leaving have a high environment satisfaction level\n\nWhat can you deduce about the interaction between Work Life Balance and Attrition?\n\nAnswer: Those that are staying have a lower density of 2’s and 3’s"
  },
  {
    "objectID": "content/01_journal/AutomatedML_I.html#performance-data-job-involvment-performance-rating",
    "href": "content/01_journal/AutomatedML_I.html#performance-data-job-involvment-performance-rating",
    "title": "Automated Machine Learning with H2O I",
    "section": "\n2.3 Performance Data: Job Involvment, Performance Rating",
    "text": "2.3 Performance Data: Job Involvment, Performance Rating\n\nemployee_attrition_tbl %>%\n    select(Attrition, contains(\"performance\"), contains(\"involvement\")) %>%\n    plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat Can you deduce about the interaction between Job Involvement and Attrition?\n\nAnswer: Those that are staying have a lower density of 2’s and 3’s"
  },
  {
    "objectID": "content/01_journal/AutomatedML_I.html#work-life-features",
    "href": "content/01_journal/AutomatedML_I.html#work-life-features",
    "title": "Automated Machine Learning with H2O I",
    "section": "\n2.4 Work-Life Features",
    "text": "2.4 Work-Life Features\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Over Time and Attrition?\n\nAnswer: The proportion of those leaving that are working Over Time are high compared to those that are not leaving"
  },
  {
    "objectID": "content/01_journal/AutomatedML_I.html#training-and-education",
    "href": "content/01_journal/AutomatedML_I.html#training-and-education",
    "title": "Automated Machine Learning with H2O I",
    "section": "\n2.5 Training and Education",
    "text": "2.5 Training and Education\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Training Times Last Year and Attrition?\n\nAnswer: People that leave tend to have more annual training"
  },
  {
    "objectID": "content/01_journal/AutomatedML_I.html#time-based-features-years-at-company-years-in-current-role",
    "href": "content/01_journal/AutomatedML_I.html#time-based-features-years-at-company-years-in-current-role",
    "title": "Automated Machine Learning with H2O I",
    "section": "\n2.6 Time-Based Features: Years at company, years in current role",
    "text": "2.6 Time-Based Features: Years at company, years in current role\n\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"years\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\nWhat can you deduce about the interaction between Years At Company and Attrition?\n\nAnswer: People that leave tend to have more working years at the company\n\nWhat can you deduce about the interaction between Years Since Last Promotion and Attrition?\n\nAnswer: It’s difficult to deduce anything based on the visualization"
  },
  {
    "objectID": "content/01_journal/Chapter_1_Challenge.html",
    "href": "content/01_journal/Chapter_1_Challenge.html",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "Your organization wants to know which companies are similar to each other to help in identifying potential customers of a SAAS software solution (e.g. Salesforce CRM or equivalent) in various segments of the market. The Sales Department is very interested in this analysis, which will help them more easily penetrate various market segments.\nYou will be using stock prices in this analysis. You come up with a method to classify companies based on how their stocks trade using their daily stock returns (percentage movement from one day to the next). This analysis will help your organization determine which companies are related to each other (competitors and have similar attributes).\nYou can analyze the stock prices using what you’ve learned in the unsupervised learning tools including K-Means and UMAP. You will use a combination of kmeans() to find groups and umap() to visualize similarity of daily stock returns."
  },
  {
    "objectID": "content/01_journal/Chapter_1_Challenge.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "href": "content/01_journal/Chapter_1_Challenge.html#step-1---convert-stock-prices-to-a-standardized-format-daily-returns",
    "title": "Machine Learning Fundamentals",
    "section": "\n5.1 Step 1 - Convert stock prices to a standardized format (daily returns)",
    "text": "5.1 Step 1 - Convert stock prices to a standardized format (daily returns)\nWhat you first need to do is get the data in a format that can be converted to a “user-item” style matrix. The challenge here is to connect the dots between what we have and what we need to do to format it properly.\nWe know that in order to compare the data, it needs to be standardized or normalized. Why? Because we cannot compare values (stock prices) that are of completely different magnitudes. In order to standardize, we will convert from adjusted stock price (dollar value) to daily returns (percent change from previous day). Here is the formula.\n\\[\nreturn_{daily} = \\frac{price_{i}-price_{i-1}}{price_{i-1}}\n\\]\nFirst, what do we have? We have stock prices for every stock in the SP 500 Index, which is the daily stock prices for over 500 stocks. The data set is over 1.2M observations.\n\nsp_500_prices_tbl %>% glimpse()\n\n#> Rows: 1,225,765\n#> Columns: 8\n#> $ symbol   <chr> \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT\", \"MSFT…\n#> $ date     <date> 2009-01-02, 2009-01-05, 2009-01-06, 2009-01-07, 2009-01-08, …\n#> $ open     <dbl> 19.53, 20.20, 20.75, 20.19, 19.63, 20.17, 19.71, 19.52, 19.53…\n#> $ high     <dbl> 20.40, 20.67, 21.00, 20.29, 20.19, 20.30, 19.79, 19.99, 19.68…\n#> $ low      <dbl> 19.37, 20.06, 20.61, 19.48, 19.55, 19.41, 19.30, 19.52, 19.01…\n#> $ close    <dbl> 20.33, 20.52, 20.76, 19.51, 20.12, 19.52, 19.47, 19.82, 19.09…\n#> $ volume   <dbl> 50084000, 61475200, 58083400, 72709900, 70255400, 49815300, 5…\n#> $ adjusted <dbl> 15.86624, 16.01451, 16.20183, 15.22628, 15.70234, 15.23408, 1…\n\n\nYour first task is to convert to a tibble named sp_500_daily_returns_tbl by performing the following operations:\n\nSelect the symbol, date and adjusted columns\nFilter to dates beginning in the year 2018 and beyond.\nCompute a Lag of 1 day on the adjusted stock price. Be sure to group by symbol first, otherwise we will have lags computed using values from the previous stock in the data frame.\nRemove a NA values from the lagging operation\nCompute the difference between adjusted and the lag\nCompute the percentage difference by dividing the difference by that lag. Name this column pct_return.\nReturn only the symbol, date, and pct_return columns\nSave as a variable named sp_500_daily_returns_tbl\n\n\n\n# Apply your data transformation skills!\n\nsp_500_daily_returns_tbl <- sp_500_prices_tbl %>%\n  select(symbol, date, adjusted) %>%\n  filter(year(date) >= 2018) %>%\n  group_by(symbol) %>%\n  mutate(lag_adjusted = lag(adjusted, 1)) %>%\n  na.omit() %>%\n  mutate(difference = adjusted - lag_adjusted,\n         pct_return = difference / lag_adjusted) %>%\n  select(symbol, date, pct_return)\n\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n# Output: sp_500_daily_returns_tbl"
  },
  {
    "objectID": "content/01_journal/Chapter_1_Challenge.html#step-2---convert-to-user-item-format",
    "href": "content/01_journal/Chapter_1_Challenge.html#step-2---convert-to-user-item-format",
    "title": "Machine Learning Fundamentals",
    "section": "\n5.2 Step 2 - Convert to User-Item Format",
    "text": "5.2 Step 2 - Convert to User-Item Format\nThe next step is to convert to a user-item format with the symbol in the first column and every other column the value of the daily returns (pct_return) for every stock at each date.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nsp_500_daily_returns_tbl <- read_rds(\"../../assets/DataSets/sp_500_daily_returns_tbl.rds\")\nsp_500_daily_returns_tbl\n\n\n\n  \n\n\n\nNow that we have the daily returns (percentage change from one day to the next), we can convert to a user-item format. The user in this case is the symbol (company), and the item in this case is the pct_return at each date.\n\nSpread the date column to get the values as percentage returns. Make sure to fill an NA values with zeros.\nSave the result as stock_date_matrix_tbl\n\n\n\n# Convert to User-Item Format\n\nstock_date_matrix_tbl <- sp_500_daily_returns_tbl %>%\n  spread(date, pct_return, fill = 0)\n\nstock_date_matrix_tbl\n\n\n\n  \n\n\n# Output: stock_date_matrix_tbl"
  },
  {
    "objectID": "content/01_journal/Chapter_1_Challenge.html#step-3---perform-k-means-clustering",
    "href": "content/01_journal/Chapter_1_Challenge.html#step-3---perform-k-means-clustering",
    "title": "Machine Learning Fundamentals",
    "section": "\n5.3 Step 3 - Perform K-Means Clustering",
    "text": "5.3 Step 3 - Perform K-Means Clustering\nNext, we’ll perform K-Means clustering.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nstock_date_matrix_tbl <- read_rds(\"../../assets/DataSets/stock_date_matrix_tbl.rds\")\nstock_date_matrix_tbl\n\n\n\n  \n\n\n\nBeginning with the stock_date_matrix_tbl, perform the following operations:\n\nDrop the non-numeric column, symbol\n\nPerform kmeans() with centers = 4 and nstart = 20\n\nSave the result as kmeans_obj\n\n\n\n# Create kmeans_obj for 4 centers\nstock_date_matrix_numeric <- stock_date_matrix_tbl %>%\n  select(-symbol)\n\n# Perform k-means clustering\nkmeans_obj <- kmeans(stock_date_matrix_numeric, centers = 4, nstart = 20)\n\nkmeans_obj\n\n#> K-means clustering with 4 clusters of sizes 71, 30, 84, 317\n#> \n#> Cluster means:\n#>     2018-01-03   2018-01-04   2018-01-05  2018-01-08    2018-01-09\n#> 1  0.014025528  0.004891946 0.0092884855 0.008080790 -0.0003389287\n#> 2  0.014549433  0.011343742 0.0006474687 0.005827603 -0.0061031977\n#> 3 -0.003333954 -0.006912672 0.0012340690 0.005406745 -0.0089961936\n#> 4  0.005217181  0.004542594 0.0065882996 0.001682355  0.0035926461\n#>      2018-01-10   2018-01-11   2018-01-12   2018-01-16  2018-01-17   2018-01-18\n#> 1 -0.0020594364  0.009308274  0.009571419 -0.008449825 0.015404285  0.001998036\n#> 2  0.0012472656  0.023712257  0.010296386 -0.014205000 0.008879761 -0.009114860\n#> 3 -0.0120427739 -0.004826786 -0.005236483  0.001309974 0.007494267 -0.006708276\n#> 4 -0.0003417889  0.011272867  0.008286783 -0.007601889 0.007489971 -0.001424046\n#>    2018-01-19  2018-01-22    2018-01-23   2018-01-24    2018-01-25   2018-01-26\n#> 1 0.006715878 0.009735809  0.0093980814 -0.007636793 -4.868228e-03 0.0166806037\n#> 2 0.001324402 0.024772066 -0.0005949105 -0.006334400 -1.029610e-02 0.0060671176\n#> 3 0.004566404 0.006985181  0.0080806520 -0.003852982  5.819039e-03 0.0008990918\n#> 4 0.007923882 0.004611634  0.0009158796  0.002736219  6.261335e-05 0.0112167041\n#>     2018-01-29   2018-01-30   2018-01-31    2018-02-01  2018-02-02  2018-02-05\n#> 1 -0.005021227 -0.009723839  0.005783896  0.0021775377 -0.02295550 -0.03846852\n#> 2 -0.015217440 -0.021314151  0.001732506  0.0088676780 -0.03741749 -0.03711698\n#> 3 -0.011666174 -0.003053078  0.010838715 -0.0147069869 -0.01321817 -0.02624758\n#> 4 -0.006083350 -0.011773883 -0.003893409 -0.0002198492 -0.01945987 -0.03929605\n#>     2018-02-06    2018-02-07  2018-02-08  2018-02-09  2018-02-12   2018-02-13\n#> 1  0.025070219 -0.0074124405 -0.04115078  0.02162704 0.018492458  0.008702354\n#> 2  0.013342351 -0.0190853955 -0.03955006 -0.00291801 0.022509484 -0.006038450\n#> 3 -0.000769148 -0.0052130982 -0.02013127  0.01909034 0.005043885  0.003804489\n#> 4  0.014327389  0.0006988956 -0.03645141  0.01201566 0.012262163  0.002250963\n#>     2018-02-14   2018-02-15    2018-02-16    2018-02-20   2018-02-21\n#> 1  0.024552532  0.016512878 -0.0064207255  0.0060618583 -0.004441781\n#> 2  0.028709277 -0.005064078 -0.0050865513  0.0004252287 -0.016731442\n#> 3 -0.004153567  0.016542153  0.0050883535 -0.0117393604 -0.016373701\n#> 4  0.017405799  0.009018172  0.0009879948 -0.0076134581 -0.002297952\n#>     2018-02-22 2018-02-23   2018-02-26   2018-02-27   2018-02-28   2018-03-01\n#> 1 -0.002997867 0.02195730 1.220083e-02 -0.009308126 -0.005461278 -0.014143419\n#> 2  0.009769590 0.02115968 9.924278e-05 -0.014163508 -0.023712904  0.001114025\n#> 3  0.007334973 0.01800925 2.523189e-03 -0.016890593 -0.004953530 -0.000381493\n#> 4 -0.001782200 0.01341795 8.558352e-03 -0.013163471 -0.012030574 -0.013488875\n#>    2018-03-02 2018-03-05    2018-03-06   2018-03-07  2018-03-08  2018-03-09\n#> 1 0.021342169 0.01379969  0.0087369561  0.007337585 0.008156522 0.017801101\n#> 2 0.007737213 0.01108851  0.0003806278 -0.007205835 0.001223280 0.021309965\n#> 3 0.001200669 0.01370457 -0.0019166661 -0.002145171 0.007270226 0.004711574\n#> 4 0.005346057 0.01038346  0.0061748492 -0.001323156 0.001313594 0.017891556\n#>      2018-03-12   2018-03-13    2018-03-14   2018-03-15   2018-03-16\n#> 1  0.0027135945 -0.012076285  0.0002114269 -0.002123033 0.0008341013\n#> 2 -0.0005388962 -0.008199908 -0.0017622280 -0.012297035 0.0129985324\n#> 3  0.0031507710  0.001106390  0.0009870808 -0.002700715 0.0054389622\n#> 4 -0.0035181932 -0.003675462 -0.0084294900 -0.001428887 0.0043333233\n#>     2018-03-19   2018-03-20    2018-03-21   2018-03-22   2018-03-23 2018-03-26\n#> 1 -0.017248805  0.004318024 -0.0017923612 -0.026251915 -0.025754376 0.03407693\n#> 2 -0.017670238  0.013784848  0.0337207010 -0.021459951 -0.007163383 0.01896047\n#> 3 -0.007648673 -0.003879733 -0.0090580220 -0.003464591 -0.013739589 0.01267397\n#> 4 -0.010398300  0.000171361  0.0004388795 -0.027250330 -0.019522841 0.02449549\n#>     2018-03-27    2018-03-28  2018-03-29  2018-04-02  2018-04-03  2018-04-04\n#> 1 -0.032635868 -0.0121534871 0.018762622 -0.02940422 0.012355715 0.015777681\n#> 2 -0.013465892 -0.0233216532 0.023735936 -0.02377674 0.019605101 0.001227871\n#> 3  0.007715361  0.0112983042 0.004369175 -0.01422130 0.007764893 0.010279533\n#> 4 -0.013135617 -0.0004245702 0.013363731 -0.02238548 0.012621133 0.012434839\n#>    2018-04-05   2018-04-06    2018-04-09   2018-04-10    2018-04-11\n#> 1 0.001453854 -0.029807187  0.0072958976  0.025353351 -0.0056141513\n#> 2 0.019447924 -0.022406930  0.0025547209  0.038827519  0.0130939406\n#> 3 0.002828578 -0.007734639 -0.0001039607 -0.001881378 -0.0007348127\n#> 4 0.007176211 -0.023302068  0.0013397648  0.015147638 -0.0067400406\n#>     2018-04-12   2018-04-13  2018-04-16  2018-04-17    2018-04-18   2018-04-19\n#> 1  0.015962569 -0.007832302 0.004860507 0.022161608  0.0002988631 -0.018124119\n#> 2  0.001740217  0.013155802 0.008616509 0.006042663  0.0206900471  0.001786194\n#> 3 -0.010752601  0.005563910 0.008682951 0.007863409 -0.0046798570 -0.013599108\n#> 4  0.008179894 -0.005095188 0.011016656 0.006195900  0.0032189057 -0.003267487\n#>     2018-04-20   2018-04-23  2018-04-24    2018-04-25  2018-04-26   2018-04-27\n#> 1 -0.010631547 -0.003682519 -0.01618772 -0.0050286856 0.020824552 -0.004572574\n#> 2 -0.003468391  0.006591688 -0.01599992  0.0059983491 0.011457675 -0.009243902\n#> 3 -0.012187591  0.001157400  0.00221505  0.0002285739 0.010261824  0.011958287\n#> 4 -0.005666901  0.002505648 -0.01123779  0.0039772566 0.003569482  0.003206567\n#>      2018-04-30    2018-05-01   2018-05-02   2018-05-03   2018-05-04\n#> 1 -4.215671e-03  0.0119120995 -0.003166279  0.004031093 0.0152252115\n#> 2  5.650547e-05 -0.0050385450  0.008551843 -0.003354957 0.0002403534\n#> 3 -5.828280e-03  0.0007776130 -0.009884618  0.001865179 0.0115802919\n#> 4 -1.188822e-02 -0.0005850741 -0.010327390 -0.005367388 0.0118618730\n#>      2018-05-07    2018-05-08  2018-05-09  2018-05-10   2018-05-11\n#> 1  0.0107081957  0.0042713777 0.018287598 0.012255246 -0.004284660\n#> 2 -0.0006381188  0.0103725962 0.019005395 0.003092002 -0.001898366\n#> 3 -0.0003423209 -0.0117845845 0.002356665 0.008776411 -0.001332108\n#> 4  0.0006844511  0.0007354292 0.006015187 0.008198444  0.003294055\n#>      2018-05-14   2018-05-15   2018-05-16   2018-05-17    2018-05-18\n#> 1  0.0016992322 -0.004501408  0.008611112 -0.004584975 -0.0022359815\n#> 2  0.0057983409  0.004290063  0.008291791  0.020700579 -0.0059074763\n#> 3 -0.0055922557 -0.011670012 -0.002364305 -0.004893340 -0.0039987220\n#> 4  0.0005690565 -0.003929441  0.006577711  0.001955032 -0.0005493316\n#>    2018-05-21   2018-05-22    2018-05-23    2018-05-24    2018-05-25\n#> 1 0.005922644 -0.003192814  0.0057628973  1.489904e-03  0.0006639455\n#> 2 0.008283126 -0.013478278 -0.0044105608 -1.261852e-02 -0.0266239832\n#> 3 0.005779365  0.002482762  0.0085556910  1.298424e-03  0.0047986326\n#> 4 0.005961593 -0.004840369  0.0006489749  8.611148e-05 -0.0012343964\n#>     2018-05-29  2018-05-30   2018-05-31   2018-06-01   2018-06-04   2018-06-05\n#> 1 -0.006059243 0.009546631 -0.001903148  0.022277656  0.003563116  0.007483896\n#> 2 -0.001696881 0.028669361 -0.012173706  0.002155198 -0.012253263 -0.002154042\n#> 3  0.001238291 0.011695739 -0.007053274 -0.003547776  0.003162351 -0.005049502\n#> 4 -0.015959552 0.012555582 -0.010942939  0.010095531  0.005981400  0.003198089\n#>     2018-06-06   2018-06-07   2018-06-08  2018-06-11   2018-06-12    2018-06-13\n#> 1  0.008494834 -0.013241385  0.004352640 0.002487955  0.008414511  0.0006958373\n#> 2  0.003809198  0.016374668 -0.004469895 0.005602012 -0.005129724 -0.0031941768\n#> 3 -0.005781605  0.002795166  0.005472490 0.001419442  0.008011873 -0.0102443629\n#> 4  0.011968862  0.001149033  0.005784347 0.001753974  0.001399618 -0.0055633906\n#>      2018-06-14   2018-06-15    2018-06-18    2018-06-19  2018-06-20\n#> 1  0.0095731668 -0.001900661  0.0009221515 -0.0088464161 0.006518084\n#> 2 -0.0051560680 -0.022707272  0.0173805699  0.0006035218 0.010461839\n#> 3  0.0089727967  0.005189750 -0.0037548361  0.0055750955 0.004292090\n#> 4  0.0009142176  0.001952892 -0.0026534461 -0.0061898455 0.001076239\n#>     2018-06-21   2018-06-22   2018-06-25   2018-06-26   2018-06-27   2018-06-28\n#> 1 -0.010378518 -0.007791614 -0.025003868  0.003632608 -0.020827173  0.010604506\n#> 2 -0.022917019  0.024278987 -0.026779988  0.017510326  0.015372888 -0.003154605\n#> 3  0.003094840  0.008221106  0.009402708  0.000226482 -0.001854346  0.007254731\n#> 4 -0.005383937  0.001021374 -0.011979354 -0.001118698 -0.010185976  0.002804796\n#>      2018-06-29   2018-07-02   2018-07-03   2018-07-05  2018-07-06  2018-07-09\n#> 1  3.736572e-03  0.007468319 -0.011619902  0.013982441 0.012671282  0.00729805\n#> 2  6.801777e-03 -0.016776871  0.009084328 -0.001488715 0.013008260  0.01913839\n#> 3  6.340609e-04 -0.002681042  0.004025346  0.012383700 0.005028909 -0.01604752\n#> 4 -6.855821e-05  0.001471821 -0.001898671  0.006614258 0.006908309  0.01402647\n#>    2018-07-10   2018-07-11   2018-07-12    2018-07-13   2018-07-16   2018-07-17\n#> 1 0.003275802 -0.008306789 1.727150e-02 -0.0023338092 -0.002482895  0.008792052\n#> 2 0.003473632 -0.024131440 7.366942e-05  0.0050874219 -0.015230587 -0.001992870\n#> 3 0.006173772  0.002633196 7.831687e-04  0.0004614193 -0.004724125 -0.001728087\n#> 4 0.001312620 -0.009738650 4.563560e-03  0.0012950176 -0.001679823  0.004588254\n#>     2018-07-18   2018-07-19   2018-07-20   2018-07-23    2018-07-24  2018-07-25\n#> 1  0.002761209 -0.004980070 -0.004154766  0.002027684 -0.0080874714 0.016187298\n#> 2  0.001663189 -0.001796304 -0.003616946 -0.003431901  0.0104494824 0.008122618\n#> 3 -0.007769495  0.008153929 -0.005246509 -0.004892180 -0.0005005501 0.007607577\n#> 4  0.005521811 -0.001518489 -0.002652620  0.001778067 -0.0004441636 0.007192060\n#>      2018-07-26   2018-07-27   2018-07-30   2018-07-31   2018-08-01\n#> 1 -0.0006751857 -0.020919476 -0.021618702  0.002895817 -0.002711583\n#> 2  0.0131985204  0.001280992  0.012986403 -0.002268616 -0.015730558\n#> 3  0.0079688330 -0.005027542 -0.002331982  0.012574433 -0.002755748\n#> 4  0.0029787150 -0.003446809 -0.003156086  0.008038429 -0.007249964\n#>     2018-08-02    2018-08-03   2018-08-06   2018-08-07    2018-08-08\n#> 1  0.014229673 -0.0002601396 0.0082356891  0.003361766  0.0003471306\n#> 2 -0.002134323 -0.0073460616 0.0052397976  0.003959465 -0.0085479395\n#> 3  0.004132915  0.0134441240 0.0002851608 -0.003699128 -0.0047101273\n#> 4  0.002993146  0.0051696868 0.0014640732  0.003157818 -0.0006984346\n#>      2018-08-09   2018-08-10    2018-08-13  2018-08-14   2018-08-15  2018-08-16\n#> 1 -0.0008920641 -0.009182561 -0.0022671805 0.006780214 -0.015767129 0.004438533\n#> 2 -0.0108872145  0.007638768 -0.0144547914 0.004542780 -0.042438317 0.006339482\n#> 3  0.0015381073 -0.005940741  0.0008234634 0.004398117  0.008029299 0.009135530\n#> 4 -0.0007957794 -0.008728654 -0.0054871905 0.010200628 -0.006691091 0.008967913\n#>     2018-08-17   2018-08-20   2018-08-21   2018-08-22   2018-08-23  2018-08-24\n#> 1 -0.001760454 4.049175e-03  0.008199431  0.006730186  0.001981029 0.015124691\n#> 2  0.003224037 8.421130e-03  0.008349307  0.013378383 -0.005292100 0.008073458\n#> 3  0.007732354 2.692069e-07 -0.009589298 -0.006575943 -0.001255369 0.004500810\n#> 4  0.006126832 5.942634e-03  0.005122876 -0.003894045 -0.003921203 0.003449513\n#>     2018-08-27    2018-08-28  2018-08-29   2018-08-30    2018-08-31\n#> 1  0.009845194  3.794339e-03 0.007306630 -0.006688627  0.0006588377\n#> 2  0.009180901 -5.525657e-03 0.007257279 -0.002890487 -0.0073978491\n#> 3 -0.002951911  1.332308e-03 0.002524474 -0.002927028  0.0011652746\n#> 4  0.007595955  4.536910e-06 0.003219045 -0.007957643  0.0014553869\n#>      2018-09-04   2018-09-05   2018-09-06   2018-09-07   2018-09-10\n#> 1 -0.0009675634 -0.018543501 -0.011209654 -0.004386366 0.0087879660\n#> 2 -0.0073232978 -0.004506993 -0.019382860 -0.004880020 0.0006737391\n#> 3 -0.0024738521  0.010518901  0.005232278 -0.009005451 0.0056867858\n#> 4 -0.0012439741  0.002183078 -0.000759536 -0.001954284 0.0029803154\n#>      2018-09-11   2018-09-12    2018-09-13   2018-09-14    2018-09-17\n#> 1  0.0018295441 -0.002137449  6.462456e-03  0.001758224 -0.0163136879\n#> 2  0.0122762856  0.009381701 -2.306182e-06  0.004538664  0.0006103131\n#> 3 -0.0019538731  0.003579706  4.775531e-03 -0.005895538  0.0034210890\n#> 4  0.0005049789  0.001161493  4.018751e-03  0.003053672 -0.0031778985\n#>     2018-09-18   2018-09-19   2018-09-20    2018-09-21   2018-09-24\n#> 1  0.010072334 -0.002499493  0.012692587 -0.0003286405  0.006489045\n#> 2  0.011631923  0.002892712 -0.004188770  0.0082752213  0.013828416\n#> 3 -0.005822705 -0.012892316  0.006698837  0.0026245042 -0.014558807\n#> 4  0.005551467  0.004069505  0.006322862  0.0003726642 -0.010072955\n#>      2018-09-25   2018-09-26    2018-09-27    2018-09-28   2018-10-01\n#> 1 -0.0008218263 -0.002833221  0.0055217433  0.0020502699 -0.004922243\n#> 2  0.0079921453 -0.009821787 -0.0002961086  0.0005533699  0.013778724\n#> 3 -0.0056531970 -0.008520217  0.0010554973  0.0117954826 -0.004743913\n#> 4 -0.0034740979 -0.002645426 -0.0009807794 -0.0007368244  0.001618323\n#>      2018-10-02   2018-10-03    2018-10-04   2018-10-05   2018-10-08\n#> 1 -0.0096354843  0.003528746 -0.0233264347 -0.017356286 -0.016146227\n#> 2 -0.0008793832  0.013208412 -0.0075673719 -0.002222662 -0.004422415\n#> 3  0.0063449319 -0.010963752  0.0005911404  0.007115132  0.011777449\n#> 4 -0.0021978236  0.001595167 -0.0056339596 -0.005892240  0.002315831\n#>     2018-10-09  2018-10-10  2018-10-11   2018-10-12    2018-10-15 2018-10-16\n#> 1 -0.002245737 -0.04697040 -0.01005558 0.0285292180 -0.0109009681 0.03496553\n#> 2  0.011660485 -0.04102685 -0.03066850 0.0059181717 -0.0069465232 0.01198762\n#> 3  0.001382168 -0.01068745 -0.02451301 0.0009265891  0.0063396100 0.01495123\n#> 4 -0.008888514 -0.02905836 -0.02220585 0.0079626327  0.0003925018 0.01900308\n#>      2018-10-17    2018-10-18   2018-10-19   2018-10-22   2018-10-23\n#> 1 -0.0001182299 -0.0250782664 -0.012541599  0.003844884 -0.003859615\n#> 2 -0.0121314041 -0.0110406026 -0.013063143 -0.010377735 -0.029278040\n#> 3 -0.0014392965  0.0003157027  0.014528266 -0.009878428  0.002693487\n#> 4 -0.0022031050 -0.0143254165 -0.004171677 -0.006903643 -0.006763992\n#>    2018-10-24   2018-10-25   2018-10-26   2018-10-29 2018-10-30   2018-10-31\n#> 1 -0.05208563  0.028991360 -0.023718562 -0.018802822 0.03454404  0.021504480\n#> 2 -0.04585749  0.015249528 -0.009554609 -0.024947273 0.02341653  0.006087733\n#> 3  0.01353611 -0.003340079 -0.020372884  0.014181789 0.01125114 -0.011297427\n#> 4 -0.03132646  0.015441294 -0.012819263  0.001596918 0.01998755  0.008027797\n#>    2018-11-01   2018-11-02   2018-11-05  2018-11-06  2018-11-07    2018-11-08\n#> 1 0.032081398 -0.003883827 -0.005924196 0.005474840 0.027114895 -0.0034784146\n#> 2 0.010308997 -0.009730453  0.018923407 0.002004953 0.022724471 -0.0277091571\n#> 3 0.003747117 -0.009108626  0.013772235 0.008152953 0.008792495  0.0004581358\n#> 4 0.015634660 -0.001293155  0.006938079 0.007426739 0.014704492  0.0004365969\n#>     2018-11-09   2018-11-12    2018-11-13   2018-11-14   2018-11-15\n#> 1 -0.024619687 -0.033687808  0.0041571717 -0.005272713  0.023355178\n#> 2 -0.004971914 -0.029063167 -0.0251968257  0.003142088  0.020515330\n#> 3  0.005367340  0.001904854  0.0001916946 -0.004133400 -0.004406745\n#> 4 -0.008675371 -0.015211256  0.0008174817 -0.007268626  0.008872325\n#>     2018-11-16   2018-11-19   2018-11-20   2018-11-21    2018-11-23 2018-11-26\n#> 1 -0.005298491 -0.044615610 -0.007829112  0.012297653 -2.928696e-03 0.02792190\n#> 2  0.007680349 -0.006087903 -0.040522927  0.018658251 -3.653804e-02 0.02157185\n#> 3  0.010063612  0.001588685 -0.008086658 -0.006933761 -7.387090e-06 0.00222745\n#> 4  0.002216731 -0.012261967 -0.018868549  0.007797251 -1.868033e-03 0.01391096\n#>     2018-11-27  2018-11-28    2018-11-29   2018-11-30  2018-12-03   2018-12-04\n#> 1  0.000669214 0.032255508 -0.0073804570  0.008241973 0.021707438 -0.044736643\n#> 2 -0.006230058 0.016374453  0.0053696702 -0.006236157 0.026350978 -0.035131989\n#> 3  0.005847793 0.002822177  0.0002154275  0.011292338 0.003797532 -0.008022436\n#> 4 -0.001218930 0.020257793 -0.0037995937  0.005229962 0.008915859 -0.034976107\n#>     2018-12-06   2018-12-07   2018-12-10   2018-12-11  2018-12-12   2018-12-13\n#> 1  0.008023883 -0.037844459  0.009075633  0.001617522  0.01094937 -0.009546741\n#> 2 -0.025829610 -0.007277311 -0.022566657 -0.003490909  0.01204804 -0.002388297\n#> 3  0.009630455 -0.007000921 -0.000701633  0.004753714 -0.00950557  0.008867633\n#> 4 -0.006218295 -0.023407900 -0.003215619 -0.003893997  0.00769762 -0.008218803\n#>     2018-12-14  2018-12-17   2018-12-18   2018-12-19   2018-12-20  2018-12-21\n#> 1 -0.020016222 -0.02540093  0.010933420 -0.029032982 -0.017370866 -0.03004544\n#> 2 -0.028984553 -0.01903142 -0.022103904 -0.013217782 -0.028551975 -0.01564321\n#> 3 -0.006215472 -0.03181874 -0.002335696 -0.005963248 -0.009304501 -0.01104888\n#> 4 -0.014658242 -0.01759075 -0.001704006 -0.015390412 -0.016571956 -0.01838267\n#>    2018-12-24 2018-12-26  2018-12-27    2018-12-28  2018-12-31   2019-01-02\n#> 1 -0.02366252 0.06254006 0.010954290 -0.0021091840 0.008540171  0.003843006\n#> 2 -0.04288499 0.07139516 0.003921202 -0.0097765403 0.006628301  0.019069103\n#> 3 -0.03925786 0.02519597 0.005720476  0.0005493469 0.003374524 -0.017623973\n#> 4 -0.02226213 0.04534322 0.008903106 -0.0019643956 0.009390790  0.002999051\n#>     2019-01-03 2019-01-04  2019-01-07  2019-01-08   2019-01-09  2019-01-10\n#> 1 -0.042207626 0.04949310 0.021900070 0.011169862  0.017527869 0.006892676\n#> 2 -0.007154041 0.04511775 0.020515467 0.011879849  0.021232497 0.007079545\n#> 3  0.003428785 0.01352601 0.002960760 0.014579573 -0.004437992 0.014320030\n#> 4 -0.020498572 0.03395158 0.009397651 0.009803304  0.006131962 0.004161268\n#>      2019-01-11    2019-01-14  2019-01-15    2019-01-16  2019-01-17  2019-01-18\n#> 1 -0.0000643815 -1.263161e-02 0.016148265 -0.0003493891 0.009778713 0.018933426\n#> 2 -0.0044939986 -5.779135e-05 0.004220701  0.0005469789 0.013001543 0.020885746\n#> 3  0.0006439650 -7.487770e-03 0.011233657  0.0018849468 0.005550432 0.005066536\n#> 4  0.0019697839 -2.211265e-03 0.004755527  0.0034638122 0.009621661 0.016372572\n#>     2019-01-22    2019-01-23   2019-01-24    2019-01-25   2019-01-28\n#> 1 -0.022274543 -0.0051502604  0.025276525  0.0213786649 -0.011670435\n#> 2 -0.026342440 -0.0139855530  0.002068247  0.0200460247 -0.015255646\n#> 3 -0.003042516  0.0054318391 -0.002902421 -0.0007662691  0.003788257\n#> 4 -0.013525856  0.0008376363  0.002910626  0.0104049820 -0.003409732\n#>     2019-01-29  2019-01-30    2019-01-31   2019-02-01  2019-02-04   2019-02-05\n#> 1 -0.013352553 0.025220812  0.0100370290  0.008579466 0.006612912  0.007444096\n#> 2  0.006852493 0.020060173 -0.0004307433  0.008515918 0.006715036 -0.001705462\n#> 3  0.004292367 0.007180800  0.0144310699 -0.004675601 0.004866655  0.001337625\n#> 4  0.003457064 0.007812059  0.0051356841  0.002655385 0.004837957  0.004876282\n#>      2019-02-06   2019-02-07    2019-02-08  2019-02-11    2019-02-12\n#> 1  0.0011515879 -0.016007614  0.0108905276 0.002164051  0.0187379927\n#> 2 -0.0072253794 -0.032870943 -0.0073106553 0.009675432  0.0109777684\n#> 3 -0.0032362265  0.005879985  0.0041796637 0.001155150 -0.0007469276\n#> 4 -0.0007242068 -0.005505920  0.0003808557 0.002443940  0.0144575364\n#>    2019-02-13    2019-02-14  2019-02-15\n#> 1 0.002882583  0.0008287979 0.003344893\n#> 2 0.018636127  0.0069304497 0.016988955\n#> 3 0.001301848 -0.0018576514 0.006505341\n#> 4 0.003986743 -0.0036341950 0.013290681\n#> \n#> Clustering vector:\n#>   [1] 4 1 4 1 4 4 1 4 4 1 1 4 4 4 1 3 3 3 4 4 4 3 4 4 1 4 1 4 4 4 1 1 1 4 4 4 4\n#>  [38] 3 1 1 1 4 4 4 2 2 4 4 4 3 4 3 1 3 1 4 3 4 4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4\n#>  [75] 4 4 4 3 4 3 4 4 4 4 4 4 3 1 4 4 4 4 4 3 4 4 4 4 3 3 4 4 4 4 4 3 4 3 4 4 4\n#> [112] 2 4 4 3 4 4 1 1 4 4 4 4 4 4 2 2 3 4 4 4 4 4 4 4 4 4 4 4 3 4 4 3 4 3 3 4 2\n#> [149] 4 4 1 4 4 3 4 3 4 4 4 2 3 3 3 3 4 4 3 3 1 3 4 4 3 4 2 4 1 4 2 4 3 1 4 4 4\n#> [186] 4 4 2 4 4 4 4 4 4 3 2 1 4 4 4 4 3 4 4 1 1 4 1 4 4 4 4 4 2 4 4 4 4 3 4 2 2\n#> [223] 4 4 4 4 4 4 2 1 4 4 3 4 4 4 3 4 4 4 1 4 1 1 4 1 1 4 4 1 4 4 3 1 4 4 4 4 4\n#> [260] 4 4 4 4 4 4 4 3 4 1 3 3 1 3 2 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 1 4 3 4\n#> [297] 4 1 3 3 4 4 1 3 1 4 4 3 4 4 1 4 3 4 4 4 4 3 4 2 4 2 4 1 1 4 4 4 1 1 4 2 4\n#> [334] 4 3 4 1 3 4 1 4 4 2 3 4 1 4 4 1 4 4 4 3 2 4 4 4 2 4 4 4 3 3 4 4 3 4 4 4 4\n#> [371] 4 3 3 4 4 3 4 3 4 4 3 2 4 4 2 1 4 1 4 4 3 4 4 1 4 4 4 4 4 4 4 4 4 4 3 4 4\n#> [408] 4 4 4 3 2 3 4 1 3 3 4 3 4 4 1 4 4 1 4 4 1 3 4 3 4 4 4 4 4 4 4 4 4 1 4 4 4\n#> [445] 3 1 1 1 1 4 1 1 4 3 4 4 4 4 4 4 1 4 4 1 4 4 4 2 4 3 4 1 1 3 3 4 4 4 1 3 3\n#> [482] 4 4 4 3 2 4 4 4 3 1 2 3 1 2 4 4 4 4 4 4 4\n#> \n#> Within cluster sum of squares by cluster:\n#> [1]  7.009809  1.588895  2.554867 18.051976\n#>  (between_SS / total_SS =  13.1 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\n\n\nUse glance() to get the tot.withinss.\n\n# Apply glance() to get the tot.withinss\n\ntot_withinss <- glance(kmeans_obj)$tot.withinss\n\ntot_withinss\n\n#> [1] 29.20555"
  },
  {
    "objectID": "content/01_journal/Chapter_1_Challenge.html#step-4---find-the-optimal-value-of-k",
    "href": "content/01_journal/Chapter_1_Challenge.html#step-4---find-the-optimal-value-of-k",
    "title": "Machine Learning Fundamentals",
    "section": "\n5.4 Step 4 - Find the optimal value of K",
    "text": "5.4 Step 4 - Find the optimal value of K\nNow that we are familiar with the process for calculating kmeans(), let’s use purrr to iterate over many values of “k” using the centers argument.\nWe’ll use this custom function called kmeans_mapper():\n\nkmeans_mapper <- function(center = 3) {\n    stock_date_matrix_tbl %>%\n        select(-symbol) %>%\n        kmeans(centers = center, nstart = 20)\n}\n\nApply the kmeans_mapper() and glance() functions iteratively using purrr.\n\nCreate a tibble containing column called centers that go from 1 to 30\nAdd a column named k_means with the kmeans_mapper() output. Use mutate() to add the column and map() to map centers to the kmeans_mapper() function.\nAdd a column named glance with the glance() output. Use mutate() and map() again to iterate over the column of k_means.\nSave the output as k_means_mapped_tbl\n\n\n\n# Use purrr to map\n\nk_means_mapped_tbl <- tibble(centers = 1:30) %>%\n  mutate(k_means = map(centers, ~kmeans_mapper(.)),\n         glance = map(k_means, ~glance(.)))\n\nk_means_mapped_tbl\n\n\n\n  \n\n\n# Output: k_means_mapped_tbl \n\nNext, let’s visualize the “tot.withinss” from the glance output as a Scree Plot.\n\nBegin with the k_means_mapped_tbl\n\nUnnest the glance column\nPlot the centers column (x-axis) versus the tot.withinss column (y-axis) using geom_point() and geom_line()\n\nAdd a title “Scree Plot” and feel free to style it with your favorite theme\n\n\n# Visualize Scree Plot\n\nunnested_tbl <- unnest(k_means_mapped_tbl, glance)\n\n# Create the Scree Plot\nscree_plot <- ggplot(unnested_tbl, aes(x = centers, y = tot.withinss)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Scree Plot\") +\n  theme_bw()\n\n# Print the Scree Plot\nprint(scree_plot)\n\n\n\n\n\n\n\nWe can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K."
  },
  {
    "objectID": "content/01_journal/Chapter_1_Challenge.html#step-5---apply-umap",
    "href": "content/01_journal/Chapter_1_Challenge.html#step-5---apply-umap",
    "title": "Machine Learning Fundamentals",
    "section": "\n5.5 Step 5 - Apply UMAP",
    "text": "5.5 Step 5 - Apply UMAP\nNext, let’s plot the UMAP 2D visualization to help us investigate cluster assignments.\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl <- read_rds(\"../../assets/DataSets/k_means_mapped_tbl.rds\")\n\nk_means_mapped_tbl\n\n\n\n  \n\n\n\nFirst, let’s apply the umap() function to the stock_date_matrix_tbl, which contains our user-item matrix in tibble format.\n\nStart with stock_date_matrix_tbl\n\nDe-select the symbol column\nUse the umap() function storing the output as umap_results\n\n\n\n# Apply UMAP\n\nstock_date_matrix_no_symbol <- stock_date_matrix_tbl %>%\n  select(-symbol)\n\n# Apply the umap() function\numap_results <- umap(stock_date_matrix_no_symbol)\n\numap_results \n\n#> umap embedding of 502 items in 2 dimensions\n#> object components: layout, data, knn, config\n\n\nNext, we want to combine the layout from the umap_results with the symbol column from the stock_date_matrix_tbl.\n\nStart with umap_results$layout\n\nConvert from a matrix data type to a tibble with as_tibble()\n\nBind the columns of the umap tibble with the symbol column from the stock_date_matrix_tbl.\nSave the results as umap_results_tbl.\n\n\n# Convert umap results to tibble with symbols\n\numap_layout_tbl <- as_tibble(umap_results$layout)\n\n#> Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#> `.name_repair` is omitted as of tibble 2.0.0.\n#> ℹ Using compatibility `.name_repair`.\n\n# Bind the columns of umap_layout_tbl with the 'symbol' column\numap_results_tbl <- bind_cols(stock_date_matrix_tbl$symbol, umap_layout_tbl)\n\n#> New names:\n#> • `` -> `...1`\n\numap_results_tbl\n\n\n\n  \n\n\n# Output: umap_results_tbl\n\nFinally, let’s make a quick visualization of the umap_results_tbl.\n\nPipe the umap_results_tbl into ggplot() mapping the columns to x-axis and y-axis\nAdd a geom_point() geometry with an alpha = 0.5\n\nApply theme_tq() and add a title “UMAP Projection”\n\n\n# Visualize UMAP results\n\numap_plot <- umap_results_tbl %>%\n  ggplot(aes(x = V1, y = V2)) +\n  geom_point(alpha = 0.5) +\n  labs(title = \"UMAP Projection\") +\n  theme_tq()\n\n# Print the UMAP plot\nprint(umap_plot)\n\n\n\n\n\n\n\nWe can now see that we have some clusters. However, we still need to combine the K-Means clusters and the UMAP 2D representation."
  },
  {
    "objectID": "content/01_journal/Chapter_1_Challenge.html#step-6---combine-k-means-and-umap",
    "href": "content/01_journal/Chapter_1_Challenge.html#step-6---combine-k-means-and-umap",
    "title": "Machine Learning Fundamentals",
    "section": "\n5.6 Step 6 - Combine K-Means and UMAP",
    "text": "5.6 Step 6 - Combine K-Means and UMAP\nNext, we combine the K-Means clusters and the UMAP 2D representation\nWe’re going to import the correct results first (just in case you were not able to complete the last step).\n\nk_means_mapped_tbl <- read_rds(\"../../assets/DataSets/k_means_mapped_tbl.rds\")\numap_results_tbl   <- read_rds(\"../../assets/DataSets/umap_results_tbl.rds\")\n\numap_results_tbl\n\n\n\n  \n\n\n\nFirst, pull out the K-Means for 10 Centers. Use this since beyond this value the Scree Plot flattens. Have a look at the business case to recall how that works.\n\n# Get the k_means_obj from the 10th center\n\nk_means_obj <- k_means_mapped_tbl$k_means[[10]]\n\n# Store as k_means_obj\n\nNext, we’ll combine the clusters from the k_means_obj with the umap_results_tbl.\n\nBegin with the k_means_obj\n\nAugment the k_means_obj with the stock_date_matrix_tbl to get the clusters added to the end of the tibble\nSelect just the symbol and .cluster columns\nLeft join the result with the umap_results_tbl by the symbol column\nLeft join the result with the result of sp_500_index_tbl %>% select(symbol, company, sector) by the symbol column.\nStore the output as umap_kmeans_results_tbl\n\n\n\n# Use your dplyr & broom skills to combine the k_means_obj with the umap_results_tbl\n\n\nk_means_augmented <- k_means_obj %>%\n  augment(stock_date_matrix_tbl)\n\n# Select 'symbol' and '.cluster' columns\nk_means_selected <- k_means_augmented %>%\n  select(symbol, .cluster)\n\n# Left join with umap_results_tbl by 'symbol' column\njoined_tbl <- left_join(umap_results_tbl, k_means_selected, by = \"symbol\")\n\n# Left join with sp_500_index_tbl by 'symbol' column\numap_kmeans_results_tbl <- left_join(joined_tbl, sp_500_index_tbl %>% select(symbol, company, sector), by = \"symbol\")\n\n\numap_kmeans_results_tbl \n\n\n\n  \n\n\n\nPlot the K-Means and UMAP results.\n\nBegin with the umap_kmeans_results_tbl\n\nUse ggplot() mapping V1, V2 and color = .cluster\n\nAdd the geom_point() geometry with alpha = 0.5\n\nApply colors as you desire (e.g. scale_color_manual(values = palette_light() %>% rep(3)))\n\n\n# Visualize the combined K-Means and UMAP results\n\nplot_kmeans_umap <- ggplot(umap_kmeans_results_tbl, aes(x = V1, y = V2, color = factor(.cluster))) +\n  geom_point(alpha = 0.5) +\n  scale_color_manual(values = palette_light() %>% rep(3)) +\n  labs(title = \"K-Means and UMAP Results\") +\n  theme_bw()\n\n# Print the plot\nprint(plot_kmeans_umap)\n\n\n\n\n\n\n\nCongratulations! You are done with the 1st challenge!"
  },
  {
    "objectID": "content/01_journal/Explaining_BB_Models.html",
    "href": "content/01_journal/Explaining_BB_Models.html",
    "title": "Explaining Black-Box Models with LIME",
    "section": "",
    "text": "1 Part 1\n\n# 1. Setup ----\n\n# Load Libraries \n\nlibrary(h2o)\n\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Your next step is to start H2O:\n#>     > h2o.init()\n#> \n#> For H2O package documentation, ask for help:\n#>     > ??h2o\n#> \n#> After starting H2O, you can use the Web UI at http://localhost:54321\n#> For more information visit https://docs.h2o.ai\n#> \n#> ----------------------------------------------------------------------\n\n\n#> \n#> Attaching package: 'h2o'\n\n\n#> The following objects are masked from 'package:stats':\n#> \n#>     cor, sd, var\n\n\n#> The following objects are masked from 'package:base':\n#> \n#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n#>     log10, log1p, log2, round, signif, trunc\n\nlibrary(recipes)\n\n#> Loading required package: dplyr\n\n\n#> \n#> Attaching package: 'dplyr'\n\n\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n\n\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\n\n#> \n#> Attaching package: 'recipes'\n\n\n#> The following object is masked from 'package:stats':\n#> \n#>     step\n\nlibrary(readxl)\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ forcats   1.0.0     ✔ readr     2.1.4\n#> ✔ ggplot2   3.4.2     ✔ stringr   1.5.0\n#> ✔ lubridate 1.9.2     ✔ tibble    3.2.1\n#> ✔ purrr     1.0.1     ✔ tidyr     1.3.0\n\n\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ lubridate::day()   masks h2o::day()\n#> ✖ dplyr::filter()    masks stats::filter()\n#> ✖ stringr::fixed()   masks recipes::fixed()\n#> ✖ lubridate::hour()  masks h2o::hour()\n#> ✖ dplyr::lag()       masks stats::lag()\n#> ✖ lubridate::month() masks h2o::month()\n#> ✖ lubridate::week()  masks h2o::week()\n#> ✖ lubridate::year()  masks h2o::year()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(tidyquant)\n\n#> Loading required package: PerformanceAnalytics\n#> Loading required package: xts\n#> Loading required package: zoo\n#> \n#> Attaching package: 'zoo'\n#> \n#> The following objects are masked from 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> \n#> \n#> ######################### Warning from 'xts' package ##########################\n#> #                                                                             #\n#> # The dplyr lag() function breaks how base R's lag() function is supposed to  #\n#> # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n#> # source() into this session won't work correctly.                            #\n#> #                                                                             #\n#> # Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n#> # conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n#> # dplyr from breaking base R's lag() function.                                #\n#> #                                                                             #\n#> # Code in packages is not affected. It's protected by R's namespace mechanism #\n#> # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#> #                                                                             #\n#> ###############################################################################\n#> \n#> Attaching package: 'xts'\n#> \n#> The following objects are masked from 'package:dplyr':\n#> \n#>     first, last\n#> \n#> \n#> Attaching package: 'PerformanceAnalytics'\n#> \n#> The following object is masked from 'package:graphics':\n#> \n#>     legend\n#> \n#> Loading required package: quantmod\n#> Loading required package: TTR\n#> Registered S3 method overwritten by 'quantmod':\n#>   method            from\n#>   as.zoo.data.frame zoo\n\nlibrary(lime)\n\n#> \n#> Attaching package: 'lime'\n#> \n#> The following object is masked from 'package:dplyr':\n#> \n#>     explain\n\nlibrary(parsnip)\nlibrary(yardstick)\n\n#> \n#> Attaching package: 'yardstick'\n#> \n#> The following object is masked from 'package:readr':\n#> \n#>     spec\n\nlibrary(tidymodels)\n\n#> ── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n#> ✔ broom        1.0.4     ✔ rsample      1.1.1\n#> ✔ dials        1.2.0     ✔ tune         1.1.1\n#> ✔ infer        1.0.4     ✔ workflows    1.1.3\n#> ✔ modeldata    1.1.0     ✔ workflowsets 1.0.1\n#> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n#> ✖ scales::discard() masks purrr::discard()\n#> ✖ lime::explain()   masks dplyr::explain()\n#> ✖ dplyr::filter()   masks stats::filter()\n#> ✖ xts::first()      masks dplyr::first()\n#> ✖ stringr::fixed()  masks recipes::fixed()\n#> ✖ dplyr::lag()      masks stats::lag()\n#> ✖ xts::last()       masks dplyr::last()\n#> ✖ dials::momentum() masks TTR::momentum()\n#> ✖ yardstick::spec() masks readr::spec()\n#> ✖ recipes::step()   masks stats::step()\n#> • Search for functions across packages at https://www.tidymodels.org/find/\n\n# Load Data\nemployee_attrition_tbl <- read_xlsx(\"./../../assets/DataSets/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.xlsx\")\ndefinitions_raw_tbl    <- read_excel(\"./../../assets/DataSets/data_definitions.xlsx\", sheet = 1, col_names = FALSE)\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\nprocess_hr_data_readable <- function(data, definitions_tbl) {\n  \n  definitions_list <- definitions_tbl %>%\n    fill(...1, .direction = \"down\") %>%\n    filter(!is.na(...2)) %>%\n    separate(...2, into = c(\"key\", \"value\"), sep = \" '\", remove = TRUE) %>%\n    rename(column_name = ...1) %>%\n    mutate(key = as.numeric(key)) %>%\n    mutate(value = value %>% str_replace(pattern = \"'\", replacement = \"\")) %>%\n    split(.$column_name) %>%\n    map(~ select(., -column_name)) %>%\n    map(~ mutate(., value = as_factor(value))) \n  \n  for (i in seq_along(definitions_list)) {\n    list_name <- names(definitions_list)[i]\n    colnames(definitions_list[[i]]) <- c(list_name, paste0(list_name, \"_value\"))\n  }\n  \n  data_merged_tbl <- list(HR_Data = data) %>%\n    append(definitions_list, after = 1) %>%\n    reduce(left_join) %>%\n    select(-one_of(names(definitions_list))) %>%\n    set_names(str_replace_all(names(.), pattern = \"_value\", \n                              replacement = \"\")) %>%\n    select(sort(names(.))) %>%\n    mutate_if(is.character, as.factor) %>%\n    mutate(\n      BusinessTravel = BusinessTravel %>% fct_relevel(\"Non-Travel\", \n                                                      \"Travel_Rarely\", \n                                                      \"Travel_Frequently\"),\n      MaritalStatus  = MaritalStatus %>% fct_relevel(\"Single\", \n                                                     \"Married\", \n                                                     \"Divorced\")\n    )\n  \n  return(data_merged_tbl)\n  \n}\n\n\nemployee_attrition_readable_tbl <- process_hr_data_readable(employee_attrition_tbl, definitions_raw_tbl)\n\n#> Joining with `by = join_by(Education)`\n#> Joining with `by = join_by(EnvironmentSatisfaction)`\n#> Joining with `by = join_by(JobInvolvement)`\n#> Joining with `by = join_by(JobSatisfaction)`\n#> Joining with `by = join_by(PerformanceRating)`\n#> Joining with `by = join_by(RelationshipSatisfaction)`\n#> Joining with `by = join_by(WorkLifeBalance)`\n\n# Split into test and train\nset.seed(seed = 1113)\nsplit_obj <- rsample::initial_split(employee_attrition_readable_tbl, prop = 0.85)\n\n# Assign training and test data\ntrain_readable_tbl <- training(split_obj)\ntest_readable_tbl  <- testing(split_obj)\n\n# ML Preprocessing Recipe \nrecipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%\n  step_zv(all_predictors()) %>%\n  step_mutate_at(c(\"JobLevel\", \"StockOptionLevel\"), fn = as.factor) %>% \n  prep()\n\nrecipe_obj\n\n#> \n#> ── Recipe ──────────────────────────────────────────────────────────────────────\n#> \n#> ── Inputs \n#> Number of variables by role\n#> outcome:    1\n#> predictor: 34\n#> \n#> ── Training information \n#> Training data contained 1249 data points and no incomplete rows.\n#> \n#> ── Operations \n#> • Zero variance filter removed: EmployeeCount, Over18, StandardHours | Trained\n#> • Variable mutation for: JobLevel, StockOptionLevel | Trained\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\n# 2. Models ----\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 minutes 18 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 14 days \n#>     H2O cluster name:           H2O_started_from_R_Mohamed_drs368 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.47 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\nautoml_leader <- h2o.loadModel(\"./../../assets/Models/DeepLearning_grid_1_AutoML_4_20230602_180322_model_1\")\nautoml_leader\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: deeplearning\n#> Model ID:  DeepLearning_grid_1_AutoML_4_20230602_180322_model_1 \n#> Status of Neuron Layers: predicting Attrition, 2-class classification, bernoulli distribution, CrossEntropy loss, 1,962 weights/biases, 33.3 KB, 9,361 training samples, mini-batch size 1\n#>   layer units             type dropout       l1       l2 mean_rate rate_rms\n#> 1     1    95            Input  5.00 %       NA       NA        NA       NA\n#> 2     2    20 RectifierDropout  0.00 % 0.000000 0.000000  0.170886 0.383407\n#> 3     3     2          Softmax      NA 0.000000 0.000000  0.001443 0.000402\n#>   momentum mean_weight weight_rms mean_bias bias_rms\n#> 1       NA          NA         NA        NA       NA\n#> 2 0.000000   -0.003411   0.125273  0.476819 0.061770\n#> 3 0.000000    0.099604   1.249723 -0.003959 0.016723\n#> \n#> \n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on full training frame **\n#> \n#> MSE:  0.06325354\n#> RMSE:  0.2515026\n#> LogLoss:  0.2330783\n#> Mean Per-Class Error:  0.1826679\n#> AUC:  0.9095887\n#> AUCPR:  0.7782235\n#> Gini:  0.8191774\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error      Rate\n#> No     876  33 0.036304   =33/909\n#> Yes     51 104 0.329032   =51/155\n#> Totals 927 137 0.078947  =84/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.316158   0.712329 129\n#> 2                       max f2  0.128333   0.738832 210\n#> 3                 max f0point5  0.523563   0.787476  87\n#> 4                 max accuracy  0.424451   0.923872 102\n#> 5                max precision  0.989375   1.000000   0\n#> 6                   max recall  0.000215   1.000000 398\n#> 7              max specificity  0.989375   1.000000   0\n#> 8             max absolute_mcc  0.316158   0.668485 129\n#> 9   max min_per_class_accuracy  0.111544   0.838710 221\n#> 10 max mean_per_class_accuracy  0.128333   0.847922 210\n#> 11                     max tns  0.989375 909.000000   0\n#> 12                     max fns  0.989375 153.000000   0\n#> 13                     max fps  0.000082 909.000000 399\n#> 14                     max tps  0.000215 155.000000 398\n#> 15                     max tnr  0.989375   1.000000   0\n#> 16                     max fnr  0.989375   0.987097   0\n#> 17                     max fpr  0.000082   1.000000 399\n#> 18                     max tpr  0.000215   1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on full validation frame **\n#> \n#> MSE:  0.1094026\n#> RMSE:  0.3307607\n#> LogLoss:  0.3706395\n#> Mean Per-Class Error:  0.2123165\n#> AUC:  0.8605442\n#> AUCPR:  0.6788407\n#> Gini:  0.7210884\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error     Rate\n#> No     131  16 0.108844  =16/147\n#> Yes     12  26 0.315789   =12/38\n#> Totals 143  42 0.151351  =28/185\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.260408   0.650000  41\n#> 2                       max f2  0.045290   0.708333  87\n#> 3                 max f0point5  0.349158   0.676471  32\n#> 4                 max accuracy  0.349158   0.864865  32\n#> 5                max precision  0.993065   1.000000   0\n#> 6                   max recall  0.003745   1.000000 148\n#> 7              max specificity  0.993065   1.000000   0\n#> 8             max absolute_mcc  0.349158   0.566940  32\n#> 9   max min_per_class_accuracy  0.127582   0.763158  61\n#> 10 max mean_per_class_accuracy  0.260408   0.787683  41\n#> 11                     max tns  0.993065 147.000000   0\n#> 12                     max fns  0.993065  37.000000   0\n#> 13                     max fps  0.000028 147.000000 184\n#> 14                     max tps  0.003745  38.000000 148\n#> 15                     max tnr  0.993065   1.000000   0\n#> 16                     max fnr  0.993065   0.973684   0\n#> 17                     max fpr  0.000028   1.000000 184\n#> 18                     max tpr  0.003745   1.000000 148\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.1258809\n#> RMSE:  0.354797\n#> LogLoss:  0.6982145\n#> Mean Per-Class Error:  0.2820327\n#> AUC:  0.7880301\n#> AUCPR:  0.4484105\n#> Gini:  0.5760602\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error       Rate\n#> No     754 155 0.170517   =155/909\n#> Yes     61  94 0.393548    =61/155\n#> Totals 815 249 0.203008  =216/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.043324   0.465347 237\n#> 2                       max f2  0.002429   0.598039 349\n#> 3                 max f0point5  0.715257   0.508806  80\n#> 4                 max accuracy  0.715257   0.868421  80\n#> 5                max precision  0.999992   1.000000   0\n#> 6                   max recall  0.000000   1.000000 399\n#> 7              max specificity  0.999992   1.000000   0\n#> 8             max absolute_mcc  0.687441   0.378371  85\n#> 9   max min_per_class_accuracy  0.007394   0.735484 310\n#> 10 max mean_per_class_accuracy  0.007394   0.744530 310\n#> 11                     max tns  0.999992 909.000000   0\n#> 12                     max fns  0.999992 151.000000   0\n#> 13                     max fps  0.000000 909.000000 399\n#> 14                     max tps  0.000000 155.000000 399\n#> 15                     max tnr  0.999992   1.000000   0\n#> 16                     max fnr  0.999992   0.974194   0\n#> 17                     max fpr  0.000000   1.000000 399\n#> 18                     max tpr  0.000000   1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                              mean       sd cv_1_valid cv_2_valid cv_3_valid\n#> accuracy                 0.855235 0.024053   0.859155   0.892019   0.849765\n#> auc                      0.805798 0.039807   0.739318   0.823694   0.814328\n#> err                      0.144765 0.024053   0.140845   0.107981   0.150235\n#> err_count               30.800000 5.069517  30.000000  23.000000  32.000000\n#> f0point5                 0.522850 0.092704   0.454545   0.671642   0.550459\n#> f1                       0.532978 0.076841   0.423077   0.610170   0.600000\n#> f2                       0.555967 0.110224   0.395683   0.559006   0.659341\n#> lift_top_group           5.940102 2.056993   2.448276   6.264706   6.264706\n#> logloss                  0.414162 0.042010   0.459008   0.393668   0.401740\n#> max_per_class_error      0.418298 0.159580   0.620690   0.470588   0.294118\n#> mcc                      0.461740 0.084063   0.347077   0.557935   0.518896\n#> mean_per_class_accuracy  0.742031 0.060409   0.657046   0.745153   0.791489\n#> mean_per_class_error     0.257969 0.060409   0.342954   0.254847   0.208511\n#> mse                      0.103424 0.003140   0.107143   0.098660   0.102990\n#> pr_auc                   0.520403 0.080464   0.390518   0.580461   0.592620\n#> precision                0.521739 0.118804   0.478261   0.720000   0.521739\n#> r2                       0.162364 0.085318   0.089024   0.264524   0.232246\n#> recall                   0.581702 0.159580   0.379310   0.529412   0.705882\n#> rmse                     0.321566 0.004899   0.327327   0.314102   0.320920\n#> specificity              0.902359 0.050059   0.934783   0.960894   0.877095\n#>                         cv_4_valid cv_5_valid\n#> accuracy                  0.849765   0.825472\n#> auc                       0.806806   0.844845\n#> err                       0.150235   0.174528\n#> err_count                32.000000  37.000000\n#> f0point5                  0.490798   0.446809\n#> f1                        0.500000   0.531646\n#> f2                        0.509554   0.656250\n#> lift_top_group            6.870968   7.851852\n#> logloss                   0.455116   0.361278\n#> max_per_class_error       0.483871   0.222222\n#> mcc                       0.411982   0.472809\n#> mean_per_class_accuracy   0.711361   0.805105\n#> mean_per_class_error      0.288639   0.194895\n#> mse                       0.105067   0.103259\n#> pr_auc                    0.509142   0.529274\n#> precision                 0.484849   0.403846\n#> r2                        0.155127   0.070901\n#> recall                    0.516129   0.777778\n#> rmse                      0.324140   0.321339\n#> specificity               0.906593   0.832432\n\n# 3. LIME ----\n\n# 3.1 Making Predictions ----\n\npredictions_tbl <- automl_leader %>% \n  h2o.predict(newdata = as.h2o(test_tbl)) %>%\n  as.tibble() %>%\n  bind_cols(\n    test_tbl %>%\n      select(Attrition, EmployeeNumber)\n  )\n\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl\n\n\n\n  \n\n\n## # A tibble: 220 x 5\n##    predict    No    Yes Attrition EmployeeNumber\n##    <fct>   <dbl>  <dbl> <fct>              <dbl>\n##  1 Yes     0.363 0.637  Yes                    1\n##  2 No      0.863 0.137  No                    15\n##  3 No      0.963 0.0374 No                    20\n##  4 No      0.868 0.132  No                    21\n##  5 No      0.952 0.0483 No                    38\n##  6 No      0.808 0.192  No                    49\n##  7 No      0.930 0.0696 No                    54\n##  8 Yes     0.559 0.441  No                    61\n##  9 Yes     0.412 0.588  No                    62\n## 10 No      0.936 0.0640 No                    70\n## # … with 210 more rows\n\n\n\ntest_tbl %>%\n  slice(1) %>%\n  glimpse()\n\n#> Rows: 1\n#> Columns: 32\n#> $ Age                      <dbl> 59\n#> $ BusinessTravel           <fct> Travel_Rarely\n#> $ DailyRate                <dbl> 1324\n#> $ Department               <fct> Research & Development\n#> $ DistanceFromHome         <dbl> 3\n#> $ Education                <fct> Bachelor\n#> $ EducationField           <fct> Medical\n#> $ EmployeeNumber           <dbl> 10\n#> $ EnvironmentSatisfaction  <fct> High\n#> $ Gender                   <fct> Female\n#> $ HourlyRate               <dbl> 81\n#> $ JobInvolvement           <fct> Very High\n#> $ JobLevel                 <fct> 1\n#> $ JobRole                  <fct> Laboratory Technician\n#> $ JobSatisfaction          <fct> Low\n#> $ MaritalStatus            <fct> Married\n#> $ MonthlyIncome            <dbl> 2670\n#> $ MonthlyRate              <dbl> 9964\n#> $ NumCompaniesWorked       <dbl> 4\n#> $ OverTime                 <fct> Yes\n#> $ PercentSalaryHike        <dbl> 20\n#> $ PerformanceRating        <fct> Outstanding\n#> $ RelationshipSatisfaction <fct> Low\n#> $ StockOptionLevel         <fct> 3\n#> $ TotalWorkingYears        <dbl> 12\n#> $ TrainingTimesLastYear    <dbl> 3\n#> $ WorkLifeBalance          <fct> Good\n#> $ YearsAtCompany           <dbl> 1\n#> $ YearsInCurrentRole       <dbl> 0\n#> $ YearsSinceLastPromotion  <dbl> 0\n#> $ YearsWithCurrManager     <dbl> 0\n#> $ Attrition                <fct> No\n\n# 3.2 Single Explanation ----\n\nexplainer <- train_tbl %>%\n  select(-Attrition) %>%\n  lime(\n    model           = automl_leader,\n    bin_continuous  = TRUE,\n    n_bins          = 4,\n    quantile_bins   = TRUE\n  )\n\nexplainer\n\n#> $model\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: deeplearning\n#> Model ID:  DeepLearning_grid_1_AutoML_4_20230602_180322_model_1 \n#> Status of Neuron Layers: predicting Attrition, 2-class classification, bernoulli distribution, CrossEntropy loss, 1,962 weights/biases, 33.3 KB, 9,361 training samples, mini-batch size 1\n#>   layer units             type dropout       l1       l2 mean_rate rate_rms\n#> 1     1    95            Input  5.00 %       NA       NA        NA       NA\n#> 2     2    20 RectifierDropout  0.00 % 0.000000 0.000000  0.170886 0.383407\n#> 3     3     2          Softmax      NA 0.000000 0.000000  0.001443 0.000402\n#>   momentum mean_weight weight_rms mean_bias bias_rms\n#> 1       NA          NA         NA        NA       NA\n#> 2 0.000000   -0.003411   0.125273  0.476819 0.061770\n#> 3 0.000000    0.099604   1.249723 -0.003959 0.016723\n#> \n#> \n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on training data. **\n#> ** Metrics reported on full training frame **\n#> \n#> MSE:  0.06325354\n#> RMSE:  0.2515026\n#> LogLoss:  0.2330783\n#> Mean Per-Class Error:  0.1826679\n#> AUC:  0.9095887\n#> AUCPR:  0.7782235\n#> Gini:  0.8191774\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error      Rate\n#> No     876  33 0.036304   =33/909\n#> Yes     51 104 0.329032   =51/155\n#> Totals 927 137 0.078947  =84/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.316158   0.712329 129\n#> 2                       max f2  0.128333   0.738832 210\n#> 3                 max f0point5  0.523563   0.787476  87\n#> 4                 max accuracy  0.424451   0.923872 102\n#> 5                max precision  0.989375   1.000000   0\n#> 6                   max recall  0.000215   1.000000 398\n#> 7              max specificity  0.989375   1.000000   0\n#> 8             max absolute_mcc  0.316158   0.668485 129\n#> 9   max min_per_class_accuracy  0.111544   0.838710 221\n#> 10 max mean_per_class_accuracy  0.128333   0.847922 210\n#> 11                     max tns  0.989375 909.000000   0\n#> 12                     max fns  0.989375 153.000000   0\n#> 13                     max fps  0.000082 909.000000 399\n#> 14                     max tps  0.000215 155.000000 398\n#> 15                     max tnr  0.989375   1.000000   0\n#> 16                     max fnr  0.989375   0.987097   0\n#> 17                     max fpr  0.000082   1.000000 399\n#> 18                     max tpr  0.000215   1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on validation data. **\n#> ** Metrics reported on full validation frame **\n#> \n#> MSE:  0.1094026\n#> RMSE:  0.3307607\n#> LogLoss:  0.3706395\n#> Mean Per-Class Error:  0.2123165\n#> AUC:  0.8605442\n#> AUCPR:  0.6788407\n#> Gini:  0.7210884\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error     Rate\n#> No     131  16 0.108844  =16/147\n#> Yes     12  26 0.315789   =12/38\n#> Totals 143  42 0.151351  =28/185\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.260408   0.650000  41\n#> 2                       max f2  0.045290   0.708333  87\n#> 3                 max f0point5  0.349158   0.676471  32\n#> 4                 max accuracy  0.349158   0.864865  32\n#> 5                max precision  0.993065   1.000000   0\n#> 6                   max recall  0.003745   1.000000 148\n#> 7              max specificity  0.993065   1.000000   0\n#> 8             max absolute_mcc  0.349158   0.566940  32\n#> 9   max min_per_class_accuracy  0.127582   0.763158  61\n#> 10 max mean_per_class_accuracy  0.260408   0.787683  41\n#> 11                     max tns  0.993065 147.000000   0\n#> 12                     max fns  0.993065  37.000000   0\n#> 13                     max fps  0.000028 147.000000 184\n#> 14                     max tps  0.003745  38.000000 148\n#> 15                     max tnr  0.993065   1.000000   0\n#> 16                     max fnr  0.993065   0.973684   0\n#> 17                     max fpr  0.000028   1.000000 184\n#> 18                     max tpr  0.003745   1.000000 148\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: deeplearning\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.1258809\n#> RMSE:  0.354797\n#> LogLoss:  0.6982145\n#> Mean Per-Class Error:  0.2820327\n#> AUC:  0.7880301\n#> AUCPR:  0.4484105\n#> Gini:  0.5760602\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>         No Yes    Error       Rate\n#> No     754 155 0.170517   =155/909\n#> Yes     61  94 0.393548    =61/155\n#> Totals 815 249 0.203008  =216/1064\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold      value idx\n#> 1                       max f1  0.043324   0.465347 237\n#> 2                       max f2  0.002429   0.598039 349\n#> 3                 max f0point5  0.715257   0.508806  80\n#> 4                 max accuracy  0.715257   0.868421  80\n#> 5                max precision  0.999992   1.000000   0\n#> 6                   max recall  0.000000   1.000000 399\n#> 7              max specificity  0.999992   1.000000   0\n#> 8             max absolute_mcc  0.687441   0.378371  85\n#> 9   max min_per_class_accuracy  0.007394   0.735484 310\n#> 10 max mean_per_class_accuracy  0.007394   0.744530 310\n#> 11                     max tns  0.999992 909.000000   0\n#> 12                     max fns  0.999992 151.000000   0\n#> 13                     max fps  0.000000 909.000000 399\n#> 14                     max tps  0.000000 155.000000 399\n#> 15                     max tnr  0.999992   1.000000   0\n#> 16                     max fnr  0.999992   0.974194   0\n#> 17                     max fpr  0.000000   1.000000 399\n#> 18                     max tpr  0.000000   1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                              mean       sd cv_1_valid cv_2_valid cv_3_valid\n#> accuracy                 0.855235 0.024053   0.859155   0.892019   0.849765\n#> auc                      0.805798 0.039807   0.739318   0.823694   0.814328\n#> err                      0.144765 0.024053   0.140845   0.107981   0.150235\n#> err_count               30.800000 5.069517  30.000000  23.000000  32.000000\n#> f0point5                 0.522850 0.092704   0.454545   0.671642   0.550459\n#> f1                       0.532978 0.076841   0.423077   0.610170   0.600000\n#> f2                       0.555967 0.110224   0.395683   0.559006   0.659341\n#> lift_top_group           5.940102 2.056993   2.448276   6.264706   6.264706\n#> logloss                  0.414162 0.042010   0.459008   0.393668   0.401740\n#> max_per_class_error      0.418298 0.159580   0.620690   0.470588   0.294118\n#> mcc                      0.461740 0.084063   0.347077   0.557935   0.518896\n#> mean_per_class_accuracy  0.742031 0.060409   0.657046   0.745153   0.791489\n#> mean_per_class_error     0.257969 0.060409   0.342954   0.254847   0.208511\n#> mse                      0.103424 0.003140   0.107143   0.098660   0.102990\n#> pr_auc                   0.520403 0.080464   0.390518   0.580461   0.592620\n#> precision                0.521739 0.118804   0.478261   0.720000   0.521739\n#> r2                       0.162364 0.085318   0.089024   0.264524   0.232246\n#> recall                   0.581702 0.159580   0.379310   0.529412   0.705882\n#> rmse                     0.321566 0.004899   0.327327   0.314102   0.320920\n#> specificity              0.902359 0.050059   0.934783   0.960894   0.877095\n#>                         cv_4_valid cv_5_valid\n#> accuracy                  0.849765   0.825472\n#> auc                       0.806806   0.844845\n#> err                       0.150235   0.174528\n#> err_count                32.000000  37.000000\n#> f0point5                  0.490798   0.446809\n#> f1                        0.500000   0.531646\n#> f2                        0.509554   0.656250\n#> lift_top_group            6.870968   7.851852\n#> logloss                   0.455116   0.361278\n#> max_per_class_error       0.483871   0.222222\n#> mcc                       0.411982   0.472809\n#> mean_per_class_accuracy   0.711361   0.805105\n#> mean_per_class_error      0.288639   0.194895\n#> mse                       0.105067   0.103259\n#> pr_auc                    0.509142   0.529274\n#> precision                 0.484849   0.403846\n#> r2                        0.155127   0.070901\n#> recall                    0.516129   0.777778\n#> rmse                      0.324140   0.321339\n#> specificity               0.906593   0.832432\n#> \n#> $preprocess\n#> function (x) \n#> x\n#> <bytecode: 0x000002dd39d6d538>\n#> <environment: 0x000002dd39d61740>\n#> \n#> $bin_continuous\n#> [1] TRUE\n#> \n#> $n_bins\n#> [1] 4\n#> \n#> $quantile_bins\n#> [1] TRUE\n#> \n#> $use_density\n#> [1] TRUE\n#> \n#> $feature_type\n#>                      Age           BusinessTravel                DailyRate \n#>                \"numeric\"                 \"factor\"                \"numeric\" \n#>               Department         DistanceFromHome                Education \n#>                 \"factor\"                \"numeric\"                 \"factor\" \n#>           EducationField           EmployeeNumber  EnvironmentSatisfaction \n#>                 \"factor\"                \"numeric\"                 \"factor\" \n#>                   Gender               HourlyRate           JobInvolvement \n#>                 \"factor\"                \"numeric\"                 \"factor\" \n#>                 JobLevel                  JobRole          JobSatisfaction \n#>                 \"factor\"                 \"factor\"                 \"factor\" \n#>            MaritalStatus            MonthlyIncome              MonthlyRate \n#>                 \"factor\"                \"numeric\"                \"numeric\" \n#>       NumCompaniesWorked                 OverTime        PercentSalaryHike \n#>                \"numeric\"                 \"factor\"                \"numeric\" \n#>        PerformanceRating RelationshipSatisfaction         StockOptionLevel \n#>                 \"factor\"                 \"factor\"                 \"factor\" \n#>        TotalWorkingYears    TrainingTimesLastYear          WorkLifeBalance \n#>                \"numeric\"                \"numeric\"                 \"factor\" \n#>           YearsAtCompany       YearsInCurrentRole  YearsSinceLastPromotion \n#>                \"numeric\"                \"numeric\"                \"numeric\" \n#>     YearsWithCurrManager \n#>                \"numeric\" \n#> \n#> $bin_cuts\n#> $bin_cuts$Age\n#>   0%  25%  50%  75% 100% \n#>   18   30   36   43   60 \n#> \n#> $bin_cuts$BusinessTravel\n#> NULL\n#> \n#> $bin_cuts$DailyRate\n#>   0%  25%  50%  75% 100% \n#>  102  465  797 1147 1499 \n#> \n#> $bin_cuts$Department\n#> NULL\n#> \n#> $bin_cuts$DistanceFromHome\n#>   0%  25%  50%  75% 100% \n#>    1    2    7   14   29 \n#> \n#> $bin_cuts$Education\n#> NULL\n#> \n#> $bin_cuts$EducationField\n#> NULL\n#> \n#> $bin_cuts$EmployeeNumber\n#>   0%  25%  50%  75% 100% \n#>    1  511 1040 1573 2065 \n#> \n#> $bin_cuts$EnvironmentSatisfaction\n#> NULL\n#> \n#> $bin_cuts$Gender\n#> NULL\n#> \n#> $bin_cuts$HourlyRate\n#>   0%  25%  50%  75% 100% \n#>   30   49   66   83  100 \n#> \n#> $bin_cuts$JobInvolvement\n#> NULL\n#> \n#> $bin_cuts$JobLevel\n#> NULL\n#> \n#> $bin_cuts$JobRole\n#> NULL\n#> \n#> $bin_cuts$JobSatisfaction\n#> NULL\n#> \n#> $bin_cuts$MaritalStatus\n#> NULL\n#> \n#> $bin_cuts$MonthlyIncome\n#>    0%   25%   50%   75%  100% \n#>  1051  2929  4908  8474 19999 \n#> \n#> $bin_cuts$MonthlyRate\n#>    0%   25%   50%   75%  100% \n#>  2094  8423 14470 20689 26968 \n#> \n#> $bin_cuts$NumCompaniesWorked\n#>   0%  25%  50%  75% 100% \n#>    0    1    2    4    9 \n#> \n#> $bin_cuts$OverTime\n#> NULL\n#> \n#> $bin_cuts$PercentSalaryHike\n#>   0%  25%  50%  75% 100% \n#>   11   12   14   18   25 \n#> \n#> $bin_cuts$PerformanceRating\n#> NULL\n#> \n#> $bin_cuts$RelationshipSatisfaction\n#> NULL\n#> \n#> $bin_cuts$StockOptionLevel\n#> NULL\n#> \n#> $bin_cuts$TotalWorkingYears\n#>   0%  25%  50%  75% 100% \n#>    0    6   10   15   38 \n#> \n#> $bin_cuts$TrainingTimesLastYear\n#>   0%  25%  50% 100% \n#>    0    2    3    6 \n#> \n#> $bin_cuts$WorkLifeBalance\n#> NULL\n#> \n#> $bin_cuts$YearsAtCompany\n#>   0%  25%  50%  75% 100% \n#>    0    3    5    9   37 \n#> \n#> $bin_cuts$YearsInCurrentRole\n#>   0%  25%  50%  75% 100% \n#>    0    2    3    7   18 \n#> \n#> $bin_cuts$YearsSinceLastPromotion\n#>   0%  50%  75% 100% \n#>    0    1    2   15 \n#> \n#> $bin_cuts$YearsWithCurrManager\n#>   0%  25%  50%  75% 100% \n#>    0    2    3    7   17 \n#> \n#> \n#> $feature_distribution\n#> $feature_distribution$Age\n#> \n#>         1         2         3         4 \n#> 0.2602082 0.2834267 0.2217774 0.2345877 \n#> \n#> $feature_distribution$BusinessTravel\n#> \n#>        Non-Travel     Travel_Rarely Travel_Frequently \n#>         0.1000801         0.7181745         0.1817454 \n#> \n#> $feature_distribution$DailyRate\n#> \n#>         1         2         3         4 \n#> 0.2514011 0.2489992 0.2497998 0.2497998 \n#> \n#> $feature_distribution$Department\n#> \n#>        Human Resources Research & Development                  Sales \n#>             0.04323459             0.65092074             0.30584468 \n#> \n#> $feature_distribution$DistanceFromHome\n#> \n#>         1         2         3         4 \n#> 0.2954363 0.2369896 0.2241793 0.2433947 \n#> \n#> $feature_distribution$Education\n#> \n#> Below College       College      Bachelor        Master        Doctor \n#>    0.11689351    0.18895116    0.38510809    0.27461970    0.03442754 \n#> \n#> $feature_distribution$EducationField\n#> \n#>  Human Resources    Life Sciences        Marketing          Medical \n#>       0.01761409       0.41793435       0.10888711       0.31144916 \n#>            Other Technical Degree \n#>       0.05444355       0.08967174 \n#> \n#> $feature_distribution$EmployeeNumber\n#> \n#>         1         2         3         4 \n#> 0.2506005 0.2497998 0.2497998 0.2497998 \n#> \n#> $feature_distribution$EnvironmentSatisfaction\n#> \n#>       Low    Medium      High Very High \n#> 0.1913531 0.1961569 0.3018415 0.3106485 \n#> \n#> $feature_distribution$Gender\n#> \n#>    Female      Male \n#> 0.4123299 0.5876701 \n#> \n#> $feature_distribution$HourlyRate\n#> \n#>         1         2         3         4 \n#> 0.2618094 0.2473979 0.2449960 0.2457966 \n#> \n#> $feature_distribution$JobInvolvement\n#> \n#>        Low     Medium       High  Very High \n#> 0.05684548 0.25780624 0.58927142 0.09607686 \n#> \n#> $feature_distribution$JobLevel\n#> \n#>          1          2          3          4          5 \n#> 0.36829464 0.36509207 0.14651721 0.07526021 0.04483587 \n#> \n#> $feature_distribution$JobRole\n#> \n#> Healthcare Representative           Human Resources     Laboratory Technician \n#>                0.08646918                0.03682946                0.18174540 \n#>                   Manager    Manufacturing Director         Research Director \n#>                0.06885508                0.09927942                0.05924740 \n#>        Research Scientist           Sales Executive      Sales Representative \n#>                0.18654924                0.22337870                0.05764612 \n#> \n#> $feature_distribution$JobSatisfaction\n#> \n#>       Low    Medium      High Very High \n#> 0.1873499 0.1985588 0.3018415 0.3122498 \n#> \n#> $feature_distribution$MaritalStatus\n#> \n#>    Single   Married  Divorced \n#> 0.3306645 0.4571657 0.2121697 \n#> \n#> $feature_distribution$MonthlyIncome\n#> \n#>         1         2         3         4 \n#> 0.2506005 0.2497998 0.2497998 0.2497998 \n#> \n#> $feature_distribution$MonthlyRate\n#> \n#>         1         2         3         4 \n#> 0.2506005 0.2497998 0.2497998 0.2497998 \n#> \n#> $feature_distribution$NumCompaniesWorked\n#> \n#>          1          2          3          4 \n#> 0.48118495 0.09927942 0.20496397 0.21457166 \n#> \n#> $feature_distribution$OverTime\n#> \n#>        No       Yes \n#> 0.7165733 0.2834267 \n#> \n#> $feature_distribution$PercentSalaryHike\n#> \n#>         1         2         3         4 \n#> 0.2866293 0.2738191 0.2289832 0.2105685 \n#> \n#> $feature_distribution$PerformanceRating\n#> \n#>         Low        Good   Excellent Outstanding \n#>   0.0000000   0.0000000   0.8414732   0.1585268 \n#> \n#> $feature_distribution$RelationshipSatisfaction\n#> \n#>       Low    Medium      High Very High \n#> 0.1889512 0.2161729 0.3018415 0.2930344 \n#> \n#> $feature_distribution$StockOptionLevel\n#> \n#>          0          1          2          3 \n#> 0.43554844 0.40592474 0.10168135 0.05684548 \n#> \n#> $feature_distribution$TotalWorkingYears\n#> \n#>         1         2         3         4 \n#> 0.3050440 0.3306645 0.1224980 0.2417934 \n#> \n#> $feature_distribution$TrainingTimesLastYear\n#> \n#>         1         2         3 \n#> 0.4603683 0.3306645 0.2089672 \n#> \n#> $feature_distribution$WorkLifeBalance\n#> \n#>        Bad       Good     Better       Best \n#> 0.05204163 0.22497998 0.61889512 0.10408327 \n#> \n#> $feature_distribution$YearsAtCompany\n#> \n#>         1         2         3         4 \n#> 0.3226581 0.2137710 0.2217774 0.2417934 \n#> \n#> $feature_distribution$YearsInCurrentRole\n#> \n#>          1          2          3          4 \n#> 0.46757406 0.08726982 0.27542034 0.16973579 \n#> \n#> $feature_distribution$YearsSinceLastPromotion\n#> \n#>         1         2         3 \n#> 0.6413131 0.1120897 0.2465973 \n#> \n#> $feature_distribution$YearsWithCurrManager\n#> \n#>          1          2          3          4 \n#> 0.46357086 0.09767814 0.25300240 0.18574860 \n#> \n#> \n#> attr(,\"class\")\n#> [1] \"data_frame_explainer\" \"explainer\"            \"list\"\n\nexplanation <- test_tbl %>%\n  slice(1) %>%\n  select(-Attrition) %>%\n  lime::explain(\n    \n    # Pass our explainer object\n    explainer = explainer,\n    # Because it is a binary classification model: 1\n    n_labels   = 1,\n    # number of features to be returned\n    n_features = 8,\n    # number of localized linear models\n    n_permutations = 5000,\n    # Let's start with 1\n    kernel_width   = 1\n  )\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation\n\n\n\n  \n\n\nexplanation %>%\n  as.tibble() %>%\n  select(feature:prediction) \n\n\n\n  \n\n\ng <- plot_features(explanation = explanation, ncol = 1)\n\ng\n\n\n\n\n\n\n# 3.3 Multiple Explanations ----\n\nexplanation <- test_tbl %>%\n  slice(1:20) %>%\n  select(-Attrition) %>%\n  lime::explain(\n    explainer = explainer,\n    n_labels   = 1,\n    n_features = 8,\n    n_permutations = 5000,\n    kernel_width   = 0.5\n  )\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nexplanation %>%\n  as.tibble()\n\n\n\n  \n\n\nplot_features(explanation, ncol = 4)\n\n\n\n\n\n\nplot_explanations(explanation)"
  },
  {
    "objectID": "content/01_journal/PerformanceMeasure.html",
    "href": "content/01_journal/PerformanceMeasure.html",
    "title": "Performance Measures",
    "section": "",
    "text": "1 Preperation\n\n# H2O modeling\nlibrary(h2o)\nlibrary(recipes)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(parsnip)\nlibrary(yardstick)\nlibrary(tidymodels)\n\n\nproduct_backorders_tbl          <- read_xlsx(\"./../../assets/DataSets/product_backorders.xlsx\")\nset.seed(seed = 1113)\nsplit_obj                       <- rsample::initial_split(product_backorders_tbl, prop = 0.85)\ntrain_readable_tbl              <- training(split_obj)\ntest_readable_tbl               <- testing(split_obj)\n\n\nrecipe_obj <- recipe(went_on_backorder ~., data = train_readable_tbl) %>% \n  step_zv(all_predictors()) %>% \n  step_mutate_at(in_transit_qty, local_bo_qty, fn = as.factor) %>% \n  prep()\n\n\ntrain_tbl <- bake(recipe_obj, new_data = train_readable_tbl)\ntest_tbl  <- bake(recipe_obj, new_data = test_readable_tbl)\n\n\n# Modeling\nh2o.init()\n\n#> \n#> H2O is not running yet, starting it now...\n#> \n#> Note:  In case of errors look at the following log files:\n#>     C:\\Users\\hp\\AppData\\Local\\Temp\\RtmpqUOX0g\\file6e4c68d127e8/h2o_Mohamed_started_from_r.out\n#>     C:\\Users\\hp\\AppData\\Local\\Temp\\RtmpqUOX0g\\file6e4c7f8a66e0/h2o_Mohamed_started_from_r.err\n#> \n#> \n#> Starting H2O JVM and connecting:  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         5 seconds 974 milliseconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 14 days \n#>     H2O cluster name:           H2O_started_from_R_Mohamed_yyy113 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   1.72 GB \n#>     H2O cluster total cores:    8 \n#>     H2O cluster allowed cores:  8 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.3.0 (2023-04-21 ucrt)\n\n# Split data into a training and a validation data frame\n# Setting the seed is just for reproducability\nsplit_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntrain_h2o <- split_h2o[[1]]\nvalid_h2o <- split_h2o[[2]]\ntest_h2o  <- as.h2o(test_tbl)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Set the target and predictors\ny <- \"went_on_backorder\"\nx <- setdiff(names(train_h2o), y)\n\n\nautoml_models_h2o <- h2o.automl(\n  x = x,\n  y = y,\n  training_frame    = train_h2o,\n  validation_frame  = valid_h2o,\n  leaderboard_frame = test_h2o,\n  max_runtime_secs  = 30,\n  nfolds            = 5 \n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n#> 14:03:26.56: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 14:03:26.72: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nmodels_tbl <- automl_models_h2o@leaderboard %>% \n  as_tibble() %>% \n  select(-c(mean_per_class_error, rmse, mse))\n\nmodels_tbl\n\n\n\n  \n\n\n#h2o.getModel(\"GBM_1_AutoML_2_20230604_185732\") %>% \n#  h2o.saveModel(path = \"./../../assets/Models/PerformanceMeasureModels/\")\n\n#h2o.getModel(\"StackedEnsemble_BestOfFamily_1_AutoML_2_20230604_185732\") %>% \n#  h2o.saveModel(path = \"./../../assets/Models/PerformanceMeasureModels/\")\n\n#h2o.getModel(\"StackedEnsemble_AllModels_1_AutoML_2_20230604_185732\") %>% \n#  h2o.saveModel(path = \"./../../assets/Models/PerformanceMeasureModels/\")\n\n\n2 Leaderboard Visualization\n\ndata_transformed_tbl <- automl_models_h2o@leaderboard %>%\n  as_tibble() %>%\n  select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% \n  mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n  slice(1:15) %>% \n  rownames_to_column(var = \"rowname\") %>%\n  # Visually this step will not change anything\n  # It reorders the factors under the hood\n  mutate(\n    model_id   = as_factor(model_id) %>% reorder(auc),\n    model_type = as.factor(model_type)\n  ) %>% \n  pivot_longer(cols = -c(model_id, model_type, rowname), \n               names_to = \"key\", \n               values_to = \"value\", \n               names_transform = list(key = forcats::fct_inorder)\n  ) %>% \n  mutate(model_id = paste0(rowname, \". \", model_id) %>% as_factor() %>% fct_rev())\n\n\ndata_transformed_tbl %>%\n  ggplot(aes(value, model_id, color = model_type)) +\n  geom_point(size = 3) +\n  geom_label(aes(label = round(value, 2), hjust = \"inward\")) +\n  \n  # Facet to break out logloss and auc\n  facet_wrap(~ key, scales = \"free_x\") +\n  labs(title = \"Leaderboard Metrics\",\n       subtitle = paste0(\"Ordered by: \", \"auc\"),\n       y = \"Model Postion, Model ID\", x = \"\") + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nplot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c(\"auc\", \"logloss\"), \n                                 n_max = 20, size = 4, include_lbl = TRUE) {\n  \n  # Setup inputs\n  # adjust input so that all formats are working\n  order_by <- tolower(order_by[[1]])\n  \n  leaderboard_tbl <- h2o_leaderboard %>%\n    as.tibble() %>%\n    select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% \n    mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n    rownames_to_column(var = \"rowname\") %>%\n    mutate(model_id = paste0(rowname, \". \", model_id) %>% as.factor())\n  \n  # Transformation\n  if (order_by == \"auc\") {\n    \n    data_transformed_tbl <- leaderboard_tbl %>%\n      slice(1:n_max) %>%\n      mutate(\n        model_id   = as_factor(model_id) %>% reorder(auc),\n        model_type = as.factor(model_type)\n      ) %>%\n      pivot_longer(cols = -c(model_id, model_type, rowname), \n                   names_to = \"key\", \n                   values_to = \"value\", \n                   names_transform = list(key = forcats::fct_inorder)\n      )\n    \n  } else if (order_by == \"logloss\") {\n    \n    data_transformed_tbl <- leaderboard_tbl %>%\n      slice(1:n_max) %>%\n      mutate(\n        model_id   = as_factor(model_id) %>% reorder(logloss) %>% fct_rev(),\n        model_type = as.factor(model_type)\n      ) %>%\n      pivot_longer(cols = -c(model_id, model_type, rowname), \n                   names_to = \"key\", \n                   values_to = \"value\", \n                   names_transform = list(key = forcats::fct_inorder)\n      )\n    \n  } else {\n    # If nothing is supplied\n    stop(paste0(\"order_by = '\", order_by, \"' is not a permitted option.\"))\n  }\n  \n  # Visualization\n  g <- data_transformed_tbl %>%\n    ggplot(aes(value, model_id, color = model_type)) +\n    geom_point(size = size) +\n    facet_wrap(~ key, scales = \"free_x\") +\n    labs(title = \"Leaderboard Metrics\",\n         subtitle = paste0(\"Ordered by: \", toupper(order_by)),\n         y = \"Model Postion, Model ID\", x = \"\")\n  \n  if (include_lbl) g <- g + geom_label(aes(label = round(value, 2), \n                                           hjust = \"inward\"))\n  \n  return(g)\n  \n}\n\n\n3 Tuning the model with grid search\n\ndeeplearning_h2o <- h2o.loadModel(\"./../../assets/Models/PerformanceMeasureModels/GBM_1_AutoML_2_20230604_185732\")\n\n# Take a look for the metrics on the training data set\n# For my model the total error in the confusion matrix is ~15 %\ndeeplearning_h2o\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: gbm\n#> Model ID:  GBM_1_AutoML_2_20230604_185732 \n#> Model Summary: \n#>   number_of_trees number_of_internal_trees model_size_in_bytes min_depth\n#> 1              71                       71              133682        10\n#>   max_depth mean_depth min_leaves max_leaves mean_leaves\n#> 1        15   14.56338         34         86    76.45071\n#> \n#> \n#> H2OBinomialMetrics: gbm\n#> ** Reported on training data. **\n#> \n#> MSE:  0.02778763\n#> RMSE:  0.1666962\n#> LogLoss:  0.1053642\n#> Mean Per-Class Error:  0.07011288\n#> AUC:  0.9909415\n#> AUCPR:  0.9449085\n#> Gini:  0.981883\n#> R^2:  0.7342451\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error        Rate\n#> No     11941  228 0.018736  =228/12169\n#> Yes      199 1439 0.121490   =199/1638\n#> Totals 12140 1667 0.030926  =427/13807\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.370337     0.870802 190\n#> 2                       max f2  0.220754     0.900138 237\n#> 3                 max f0point5  0.509508     0.895035 150\n#> 4                 max accuracy  0.430972     0.969436 172\n#> 5                max precision  0.984412     1.000000   0\n#> 6                   max recall  0.021484     1.000000 361\n#> 7              max specificity  0.984412     1.000000   0\n#> 8             max absolute_mcc  0.370337     0.853280 190\n#> 9   max min_per_class_accuracy  0.220754     0.953406 237\n#> 10 max mean_per_class_accuracy  0.176923     0.954080 255\n#> 11                     max tns  0.984412 12169.000000   0\n#> 12                     max fns  0.984412  1634.000000   0\n#> 13                     max fps  0.001191 12169.000000 399\n#> 14                     max tps  0.021484  1638.000000 361\n#> 15                     max tnr  0.984412     1.000000   0\n#> 16                     max fnr  0.984412     0.997558   0\n#> 17                     max fpr  0.001191     1.000000 399\n#> 18                     max tpr  0.021484     1.000000 361\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: gbm\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.0587502\n#> RMSE:  0.2423844\n#> LogLoss:  0.1990656\n#> Mean Per-Class Error:  0.1708416\n#> AUC:  0.9304435\n#> AUCPR:  0.6683467\n#> Gini:  0.860887\n#> R^2:  0.4393215\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2000 104 0.049430  =104/2104\n#> Yes      83 201 0.292254    =83/284\n#> Totals 2083 305 0.078308  =187/2388\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.295120    0.682513 175\n#> 2                       max f2  0.118418    0.744681 257\n#> 3                 max f0point5  0.501192    0.685798 112\n#> 4                 max accuracy  0.295120    0.921692 175\n#> 5                max precision  0.982717    1.000000   0\n#> 6                   max recall  0.001672    1.000000 397\n#> 7              max specificity  0.982717    1.000000   0\n#> 8             max absolute_mcc  0.295120    0.638443 175\n#> 9   max min_per_class_accuracy  0.112359    0.866197 261\n#> 10 max mean_per_class_accuracy  0.118418    0.868600 257\n#> 11                     max tns  0.982717 2104.000000   0\n#> 12                     max fns  0.982717  283.000000   0\n#> 13                     max fps  0.000916 2104.000000 399\n#> 14                     max tps  0.001672  284.000000 397\n#> 15                     max tnr  0.982717    1.000000   0\n#> 16                     max fnr  0.982717    0.996479   0\n#> 17                     max fpr  0.000916    1.000000 399\n#> 18                     max tpr  0.001672    1.000000 397\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: gbm\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.06110516\n#> RMSE:  0.2471946\n#> LogLoss:  0.2046862\n#> Mean Per-Class Error:  0.189106\n#> AUC:  0.9266475\n#> AUCPR:  0.6618482\n#> Gini:  0.853295\n#> R^2:  0.4156033\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     11452  717 0.058920   =717/12169\n#> Yes      523 1115 0.319292    =523/1638\n#> Totals 11975 1832 0.089810  =1240/13807\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.261371     0.642651 211\n#> 2                       max f2  0.105062     0.720467 284\n#> 3                 max f0point5  0.475504     0.664152 140\n#> 4                 max accuracy  0.442246     0.918302 150\n#> 5                max precision  0.981317     1.000000   0\n#> 6                   max recall  0.002155     1.000000 397\n#> 7              max specificity  0.981317     1.000000   0\n#> 8             max absolute_mcc  0.218350     0.592745 227\n#> 9   max min_per_class_accuracy  0.096255     0.852741 290\n#> 10 max mean_per_class_accuracy  0.105062     0.853773 284\n#> 11                     max tns  0.981317 12169.000000   0\n#> 12                     max fns  0.981317  1636.000000   0\n#> 13                     max fps  0.001155 12169.000000 399\n#> 14                     max tps  0.002155  1638.000000 397\n#> 15                     max tnr  0.981317     1.000000   0\n#> 16                     max fnr  0.981317     0.998779   0\n#> 17                     max fpr  0.001155     1.000000 399\n#> 18                     max tpr  0.002155     1.000000 397\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                               mean        sd cv_1_valid cv_2_valid cv_3_valid\n#> accuracy                  0.911203  0.008376   0.916727   0.919623   0.897863\n#> auc                       0.926314  0.007074   0.936941   0.928989   0.918407\n#> err                       0.088797  0.008376   0.083273   0.080377   0.102137\n#> err_count               245.200000 23.091124 230.000000 222.000000 282.000000\n#> f0point5                  0.625966  0.036199   0.675607   0.638945   0.575835\n#> f1                        0.647218  0.033202   0.698953   0.630000   0.613699\n#> f2                        0.671032  0.040828   0.723970   0.621302   0.656891\n#> lift_top_group            7.064130  0.653370   6.302183   7.390181   7.585165\n#> logloss                   0.205569  0.007513   0.202787   0.196426   0.216568\n#> max_per_class_error       0.311437  0.051078   0.258333   0.384365   0.310769\n#> mcc                       0.598884  0.035209   0.652276   0.585143   0.560131\n#> mean_per_class_accuracy   0.814730  0.022662   0.842315   0.786636   0.807464\n#> mean_per_class_error      0.185270  0.022662   0.157685   0.213364   0.192536\n#> mse                       0.061358  0.002631   0.059897   0.058984   0.065675\n#> pr_auc                    0.659116  0.043944   0.728681   0.631113   0.620452\n#> precision                 0.613034  0.042192   0.660891   0.645051   0.553086\n#> r2                        0.411786  0.037866   0.471584   0.402979   0.367629\n#> recall                    0.688563  0.051078   0.741667   0.615635   0.689231\n#> rmse                      0.247660  0.005264   0.244738   0.242866   0.256271\n#> specificity               0.940896  0.011687   0.942964   0.957638   0.925698\n#>                         cv_4_valid cv_5_valid\n#> accuracy                  0.910177   0.911626\n#> auc                       0.924848   0.922387\n#> err                       0.089823   0.088374\n#> err_count               248.000000 244.000000\n#> f0point5                  0.616753   0.622691\n#> f1                        0.634218   0.659218\n#> f2                        0.652702   0.700297\n#> lift_top_group            7.632132   6.410991\n#> logloss                   0.203478   0.208584\n#> max_per_class_error       0.334365   0.269350\n#> mcc                       0.583996   0.612873\n#> mean_per_class_accuracy   0.804105   0.833127\n#> mean_per_class_error      0.195895   0.166873\n#> mse                       0.060355   0.061878\n#> pr_auc                    0.675014   0.640320\n#> precision                 0.605634   0.600509\n#> r2                        0.415739   0.400996\n#> recall                    0.665635   0.730650\n#> rmse                      0.245672   0.248752\n#> specificity               0.942576   0.935603\n\n# We want to see how it performs for the testing data frame\ntest_tbl\n\n\n\n  \n\n\n# Make sure to convert it to an h20 object\n# Accuracy of the confusion matrix shows ~85 % accuracy\nh2o.performance(deeplearning_h2o, newdata = as.h2o(test_tbl))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n#> H2OBinomialMetrics: gbm\n#> \n#> MSE:  0.06300944\n#> RMSE:  0.2510168\n#> LogLoss:  0.2146609\n#> Mean Per-Class Error:  0.2078627\n#> AUC:  0.9141817\n#> AUCPR:  0.6572556\n#> Gini:  0.8283635\n#> R^2:  0.4048776\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2397 117 0.046539  =117/2514\n#> Yes     127 217 0.369186   =127/344\n#> Totals 2524 334 0.085374  =244/2858\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.315021    0.640118 175\n#> 2                       max f2  0.116658    0.725686 266\n#> 3                 max f0point5  0.477910    0.670530 118\n#> 4                 max accuracy  0.357509    0.917775 159\n#> 5                max precision  0.981688    1.000000   0\n#> 6                   max recall  0.001891    1.000000 396\n#> 7              max specificity  0.981688    1.000000   0\n#> 8             max absolute_mcc  0.315021    0.591781 175\n#> 9   max min_per_class_accuracy  0.109851    0.851744 270\n#> 10 max mean_per_class_accuracy  0.116658    0.855742 266\n#> 11                     max tns  0.981688 2514.000000   0\n#> 12                     max fns  0.981688  342.000000   0\n#> 13                     max fps  0.001089 2514.000000 399\n#> 14                     max tps  0.001891  344.000000 396\n#> 15                     max tnr  0.981688    1.000000   0\n#> 16                     max fnr  0.981688    0.994186   0\n#> 17                     max fpr  0.001089    1.000000 399\n#> 18                     max tpr  0.001891    1.000000 396\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n# deeplearning_grid_01 <- h2o.grid(\n#   \n#   # See help page for available algos\n#   algorithm = \"deeplearning\",\n#   \n#   # I just use the same as the object\n#   grid_id = \"deeplearning_grid_01\",\n#   \n#   # The following is for ?h2o.deeplearning()\n#   # predictor and response variables\n#   x = x,\n#   y = y,\n#   \n#   # training and validation frame and crossfold validation\n#   training_frame   = train_h2o,\n#   validation_frame = valid_h2o,\n#   nfolds = 5,\n#   \n#   # Hyperparamters: Use deeplearning_h2o@allparameters to see all\n#   hyper_params = list(\n#     # Use some combinations (the first one was the original)\n#     hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n#     epochs = c(10, 50, 100)\n#   )\n# )\n\n#deeplearning_grid_01\n\n#h2oGrid_Model <- h2o.getGrid(grid_id = \"deeplearning_grid_01\", sort_by = \"auc\", decreasing = TRUE)\n#ids <- h2oGrid_Model@model_ids\n\n#h2o.getModel(\"deeplearning_grid_01_model_1\") %>% \n#  h2o.saveModel(path = \"./../../assets/Models/\")\n\ndeeplearning_grid_01_model_1 <- h2o.loadModel(\"./../../assets/Models/deeplearning_grid_01_model_1\")\n\ndeeplearning_grid_01_model_1 %>% h2o.auc(train = T, valid = T, xval = T)\n\n#>     train     valid      xval \n#> 0.8592588 0.8224392 0.8161172\n\ndeeplearning_grid_01_model_1 %>%\n  h2o.performance(newdata = as.h2o(test_tbl))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n\n#> H2OBinomialMetrics: deeplearning\n#> \n#> MSE:  0.09097973\n#> RMSE:  0.3016285\n#> LogLoss:  0.3245311\n#> Mean Per-Class Error:  0.3076608\n#> AUC:  0.7892401\n#> AUCPR:  0.3860203\n#> Gini:  0.5784803\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No Yes    Error       Rate\n#> No     2151 363 0.144391  =363/2514\n#> Yes     162 182 0.470930   =162/344\n#> Totals 2313 545 0.183695  =525/2858\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.192572    0.409449 202\n#> 2                       max f2  0.119420    0.528154 262\n#> 3                 max f0point5  0.354165    0.409173 112\n#> 4                 max accuracy  0.664204    0.888733  40\n#> 5                max precision  0.999978    1.000000   0\n#> 6                   max recall  0.000066    1.000000 399\n#> 7              max specificity  0.999978    1.000000   0\n#> 8             max absolute_mcc  0.165257    0.318781 223\n#> 9   max min_per_class_accuracy  0.113717    0.715116 267\n#> 10 max mean_per_class_accuracy  0.119420    0.717894 262\n#> 11                     max tns  0.999978 2514.000000   0\n#> 12                     max fns  0.999978  343.000000   0\n#> 13                     max fps  0.000066 2514.000000 399\n#> 14                     max tps  0.000066  344.000000 399\n#> 15                     max tnr  0.999978    1.000000   0\n#> 16                     max fnr  0.999978    0.997093   0\n#> 17                     max fpr  0.000066    1.000000 399\n#> 18                     max tpr  0.000066    1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n# 4. Assessing Performance ----\nstacked_ensemble_h2o <- h2o.loadModel(\"./../../assets/Models/PerformanceMeasureModels/GBM_1_AutoML_2_20230604_185732\")\ndeeplearning_h2o     <- h2o.loadModel(\"./../../assets/Models/PerformanceMeasureModels/StackedEnsemble_BestOfFamily_1_AutoML_2_20230604_185732\")\nglm_h2o              <- h2o.loadModel(\"./../../assets/Models/PerformanceMeasureModels/StackedEnsemble_AllModels_1_AutoML_2_20230604_185732\")\n\n\nperformance_h2o <- h2o.performance(stacked_ensemble_h2o, newdata = as.h2o(test_tbl))\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntypeof(performance_h2o)\n\n#> [1] \"S4\"\n\nperformance_h2o %>% slotNames()\n\n#> [1] \"algorithm\" \"on_train\"  \"on_valid\"  \"on_xval\"   \"metrics\"\n\n# We are focusing on the slot metrics. This slot contains all possible metrics\nperformance_h2o@metrics\n\n#> $model\n#> $model$`__meta`\n#> $model$`__meta`$schema_version\n#> [1] 3\n#> \n#> $model$`__meta`$schema_name\n#> [1] \"ModelKeyV3\"\n#> \n#> $model$`__meta`$schema_type\n#> [1] \"Key<Model>\"\n#> \n#> \n#> $model$name\n#> [1] \"GBM_1_AutoML_2_20230604_185732\"\n#> \n#> $model$type\n#> [1] \"Key<Model>\"\n#> \n#> $model$URL\n#> [1] \"/3/Models/GBM_1_AutoML_2_20230604_185732\"\n#> \n#> \n#> $model_checksum\n#> [1] \"8466115728135137408\"\n#> \n#> $frame\n#> $frame$name\n#> [1] \"test_tbl_sid_85af_121\"\n#> \n#> \n#> $frame_checksum\n#> [1] \"-2152239742270184578\"\n#> \n#> $description\n#> NULL\n#> \n#> $scoring_time\n#> [1] 1.686485e+12\n#> \n#> $predictions\n#> NULL\n#> \n#> $MSE\n#> [1] 0.06300944\n#> \n#> $RMSE\n#> [1] 0.2510168\n#> \n#> $nobs\n#> [1] 2858\n#> \n#> $custom_metric_name\n#> NULL\n#> \n#> $custom_metric_value\n#> [1] 0\n#> \n#> $r2\n#> [1] 0.4048776\n#> \n#> $logloss\n#> [1] 0.2146609\n#> \n#> $AUC\n#> [1] 0.9141817\n#> \n#> $pr_auc\n#> [1] 0.6572556\n#> \n#> $Gini\n#> [1] 0.8283635\n#> \n#> $mean_per_class_error\n#> [1] 0.2078627\n#> \n#> $domain\n#> [1] \"No\"  \"Yes\"\n#> \n#> $cm\n#> $cm$`__meta`\n#> $cm$`__meta`$schema_version\n#> [1] 3\n#> \n#> $cm$`__meta`$schema_name\n#> [1] \"ConfusionMatrixV3\"\n#> \n#> $cm$`__meta`$schema_type\n#> [1] \"ConfusionMatrix\"\n#> \n#> \n#> $cm$table\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>          No Yes  Error          Rate\n#> No     2397 117 0.0465 = 117 / 2,514\n#> Yes     127 217 0.3692 =   127 / 344\n#> Totals 2524 334 0.0854 = 244 / 2,858\n#> \n#> \n#> $thresholds_and_metric_scores\n#> Metrics for Thresholds: Binomial metrics as a function of classification thresholds\n#>   threshold       f1       f2 f0point5 accuracy precision   recall specificity\n#> 1  0.981688 0.011561 0.007257 0.028409 0.880336  1.000000 0.005814    1.000000\n#> 2  0.979688 0.017291 0.010877 0.042135 0.880686  1.000000 0.008721    1.000000\n#> 3  0.977837 0.022989 0.014493 0.055556 0.881036  1.000000 0.011628    1.000000\n#> 4  0.968831 0.034286 0.021708 0.081522 0.881735  1.000000 0.017442    1.000000\n#> 5  0.958039 0.039886 0.025307 0.094086 0.882085  1.000000 0.020349    1.000000\n#>   absolute_mcc min_per_class_accuracy mean_per_class_accuracy  tns fns fps tps\n#> 1     0.071538               0.005814                0.502907 2514 342   0   2\n#> 2     0.087632               0.008721                0.504360 2514 341   0   3\n#> 3     0.101206               0.011628                0.505814 2514 340   0   4\n#> 4     0.123995               0.017442                0.508721 2514 338   0   6\n#> 5     0.133953               0.020349                0.510174 2514 337   0   7\n#>        tnr      fnr      fpr      tpr idx\n#> 1 1.000000 0.994186 0.000000 0.005814   0\n#> 2 1.000000 0.991279 0.000000 0.008721   1\n#> 3 1.000000 0.988372 0.000000 0.011628   2\n#> 4 1.000000 0.982558 0.000000 0.017442   3\n#> 5 1.000000 0.979651 0.000000 0.020349   4\n#> \n#> ---\n#>     threshold       f1       f2 f0point5 accuracy precision   recall\n#> 395  0.002490 0.228210 0.424715 0.156023 0.188244  0.128850 0.997093\n#> 396  0.002199 0.224844 0.420034 0.153509 0.172498  0.126709 0.997093\n#> 397  0.001891 0.221507 0.415660 0.150983 0.153954  0.124547 1.000000\n#> 398  0.001614 0.219458 0.412767 0.149461 0.143807  0.123253 1.000000\n#> 399  0.001382 0.217653 0.410207 0.148123 0.134710  0.122116 1.000000\n#> 400  0.001089 0.214866 0.406235 0.146060 0.120364  0.120364 1.000000\n#>     specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy tns\n#> 395    0.077566     0.096119               0.077566                0.537329 195\n#> 396    0.059666     0.082559               0.059666                0.528379 150\n#> 397    0.038186     0.068964               0.038186                0.519093  96\n#> 398    0.026651     0.057313               0.026651                0.513325  67\n#> 399    0.016309     0.044627               0.016309                0.508154  41\n#> 400    0.000000     0.000000               0.000000                0.500000   0\n#>     fns  fps tps      tnr      fnr      fpr      tpr idx\n#> 395   1 2319 343 0.077566 0.002907 0.922434 0.997093 394\n#> 396   1 2364 343 0.059666 0.002907 0.940334 0.997093 395\n#> 397   0 2418 344 0.038186 0.000000 0.961814 1.000000 396\n#> 398   0 2447 344 0.026651 0.000000 0.973349 1.000000 397\n#> 399   0 2473 344 0.016309 0.000000 0.983691 1.000000 398\n#> 400   0 2514 344 0.000000 0.000000 1.000000 1.000000 399\n#> \n#> $max_criteria_and_metric_scores\n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.315021    0.640118 175\n#> 2                       max f2  0.116658    0.725686 266\n#> 3                 max f0point5  0.477910    0.670530 118\n#> 4                 max accuracy  0.357509    0.917775 159\n#> 5                max precision  0.981688    1.000000   0\n#> 6                   max recall  0.001891    1.000000 396\n#> 7              max specificity  0.981688    1.000000   0\n#> 8             max absolute_mcc  0.315021    0.591781 175\n#> 9   max min_per_class_accuracy  0.109851    0.851744 270\n#> 10 max mean_per_class_accuracy  0.116658    0.855742 266\n#> 11                     max tns  0.981688 2514.000000   0\n#> 12                     max fns  0.981688  342.000000   0\n#> 13                     max fps  0.001089 2514.000000 399\n#> 14                     max tps  0.001891  344.000000 396\n#> 15                     max tnr  0.981688    1.000000   0\n#> 16                     max fnr  0.981688    0.994186   0\n#> 17                     max fpr  0.001089    1.000000 399\n#> 18                     max tpr  0.001891    1.000000 396\n#> \n#> $gains_lift_table\n#> Gains/Lift Table: Avg response rate: 12.04 %, avg score: 10.64 %\n#>    group cumulative_data_fraction lower_threshold     lift cumulative_lift\n#> 1      1               0.01014696        0.876408 6.875702        6.875702\n#> 2      2               0.02029391        0.783450 7.162189        7.018945\n#> 3      3               0.03009097        0.730624 5.934385        6.665833\n#> 4      4               0.04023793        0.676241 6.302727        6.574267\n#> 5      5               0.05003499        0.624450 5.934385        6.448975\n#> 6      6               0.10006998        0.364175 4.996503        5.722739\n#> 7      7               0.15010497        0.223126 2.440153        4.628544\n#> 8      8               0.20013996        0.134956 2.033461        3.979773\n#> 9      9               0.30020994        0.060119 0.958631        2.972726\n#> 10    10               0.39993002        0.033095 0.408119        2.333257\n#> 11    11               0.50000000        0.021110 0.145247        1.895349\n#> 12    12               0.60006998        0.013270 0.232396        1.618028\n#> 13    13               0.69979006        0.008312 0.145757        1.408230\n#> 14    14               0.79986004        0.005066 0.058099        1.239316\n#> 15    15               0.89993002        0.002984 0.058099        1.107967\n#> 16    16               1.00000000        0.000563 0.029049        1.000000\n#>    response_rate    score cumulative_response_rate cumulative_score\n#> 1       0.827586 0.920025                 0.827586         0.920025\n#> 2       0.862069 0.819222                 0.844828         0.869623\n#> 3       0.714286 0.758824                 0.802326         0.833549\n#> 4       0.758621 0.697079                 0.791304         0.799135\n#> 5       0.714286 0.650662                 0.776224         0.770063\n#> 6       0.601399 0.488914                 0.688811         0.629489\n#> 7       0.293706 0.292636                 0.557110         0.517204\n#> 8       0.244755 0.169475                 0.479021         0.430272\n#> 9       0.115385 0.092000                 0.357809         0.317515\n#> 10      0.049123 0.043922                 0.280840         0.249296\n#> 11      0.017483 0.026821                 0.228132         0.204770\n#> 12      0.027972 0.016905                 0.194752         0.173441\n#> 13      0.017544 0.010638                 0.169500         0.150241\n#> 14      0.006993 0.006627                 0.149169         0.132274\n#> 15      0.006993 0.003972                 0.133359         0.118007\n#> 16      0.003497 0.001991                 0.120364         0.106397\n#>    capture_rate cumulative_capture_rate       gain cumulative_gain\n#> 1      0.069767                0.069767 587.570168      587.570168\n#> 2      0.072674                0.142442 616.218925      601.894547\n#> 3      0.058140                0.200581 493.438538      566.583288\n#> 4      0.063953                0.264535 530.272654      557.426694\n#> 5      0.058140                0.322674 493.438538      544.897544\n#> 6      0.250000                0.572674 399.650350      472.273947\n#> 7      0.122093                0.694767 144.015287      362.854394\n#> 8      0.101744                0.796512 103.346073      297.977313\n#> 9      0.095930                0.892442  -4.136852      197.272592\n#> 10     0.040698                0.933140 -59.188086      133.325703\n#> 11     0.014535                0.947674 -85.475281       89.534884\n#> 12     0.023256                0.970930 -76.760449       61.802834\n#> 13     0.014535                0.985465 -85.424317       40.822965\n#> 14     0.005814                0.991279 -94.190112       23.931565\n#> 15     0.005814                0.997093 -94.190112       10.796729\n#> 16     0.002907                1.000000 -97.095056        0.000000\n#>    kolmogorov_smirnov\n#> 1            0.067779\n#> 2            0.138862\n#> 3            0.193819\n#> 4            0.254988\n#> 5            0.309946\n#> 6            0.537273\n#> 7            0.619191\n#> 8            0.677975\n#> 9            0.673269\n#> 10           0.606171\n#> 11           0.508931\n#> 12           0.421606\n#> 13           0.324765\n#> 14           0.217612\n#> 15           0.110458\n#> 16           0.000000\n\n# Classifier Summary Metrics\n\nh2o.auc(performance_h2o, train = T, valid = T, xval = T)\n\n#> [1] 0.9141817\n\nh2o.auc(stacked_ensemble_h2o, train = T, valid = T, xval = T)\n\n#>     train     valid      xval \n#> 0.9909415 0.9304435 0.9266475\n\nh2o.giniCoef(performance_h2o)\n\n#> [1] 0.8283635\n\nh2o.logloss(performance_h2o)\n\n#> [1] 0.2146609\n\n# result for the training data\nh2o.confusionMatrix(stacked_ensemble_h2o)\n\n\n\n  \n\n\nh2o.confusionMatrix(performance_h2o)\n\n\n\n  \n\n\n\n\n4 Visualizing the trade of between the precision and the recall and the optimal threshold\n\nperformance_tbl <- performance_h2o %>%\n  h2o.metric() %>%\n  as.tibble() \n\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n\nperformance_tbl %>% \n  glimpse()\n\n#> Rows: 400\n#> Columns: 20\n#> $ threshold               <dbl> 0.9816879, 0.9796877, 0.9778367, 0.9688306, 0.…\n#> $ f1                      <dbl> 0.01156069, 0.01729107, 0.02298851, 0.03428571…\n#> $ f2                      <dbl> 0.007256894, 0.010877447, 0.014492754, 0.02170…\n#> $ f0point5                <dbl> 0.02840909, 0.04213483, 0.05555556, 0.08152174…\n#> $ accuracy                <dbl> 0.8803359, 0.8806858, 0.8810357, 0.8817355, 0.…\n#> $ precision               <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.…\n#> $ recall                  <dbl> 0.005813953, 0.008720930, 0.011627907, 0.01744…\n#> $ specificity             <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.…\n#> $ absolute_mcc            <dbl> 0.07153841, 0.08763165, 0.10120604, 0.12399503…\n#> $ min_per_class_accuracy  <dbl> 0.005813953, 0.008720930, 0.011627907, 0.01744…\n#> $ mean_per_class_accuracy <dbl> 0.5029070, 0.5043605, 0.5058140, 0.5087209, 0.…\n#> $ tns                     <dbl> 2514, 2514, 2514, 2514, 2514, 2514, 2514, 2514…\n#> $ fns                     <dbl> 342, 341, 340, 338, 337, 336, 334, 333, 332, 3…\n#> $ fps                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 3…\n#> $ tps                     <dbl> 2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14, 15, 17, …\n#> $ tnr                     <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.…\n#> $ fnr                     <dbl> 0.9941860, 0.9912791, 0.9883721, 0.9825581, 0.…\n#> $ fpr                     <dbl> 0.0000000000, 0.0000000000, 0.0000000000, 0.00…\n#> $ tpr                     <dbl> 0.005813953, 0.008720930, 0.011627907, 0.01744…\n#> $ idx                     <int> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n\ntheme_new <- theme(\n  legend.position  = \"bottom\",\n  legend.key       = element_blank(),\n  panel.background = element_rect(fill   = \"transparent\"),\n  panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n  panel.grid.major = element_line(color = \"grey\", size = 0.333)\n) \n\n#> Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#> ℹ Please use the `linewidth` argument instead.\n\n\n#> Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#> ℹ Please use the `linewidth` argument instead.\n\nperformance_tbl %>%\n  filter(f1 == max(f1))\n\n\n\n  \n\n\nperformance_tbl %>%\n  ggplot(aes(x = threshold)) +\n  geom_line(aes(y = precision), color = \"blue\", size = 1) +\n  geom_line(aes(y = recall), color = \"red\", size = 1) +\n  \n  # Insert line where precision and recall are harmonically optimized\n  geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n  labs(title = \"Precision vs Recall\", y = \"value\") +\n  theme_new\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n5 ROC Plot\n\npath <- \"./../../assets/Models/PerformanceMeasureModels/GBM_1_AutoML_2_20230604_185732\"\n\nload_model_performance_metrics <- function(path, test_tbl) {\n  \n  model_h2o <- h2o.loadModel(path)\n  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n  \n  perf_h2o %>%\n    h2o.metric() %>%\n    as_tibble() %>%\n    mutate(auc = h2o.auc(perf_h2o)) %>%\n    select(tpr, fpr, auc)\n  \n}\n\n\nmodel_metrics_tbl <- fs::dir_info(path = \"./../../assets/Models/PerformanceMeasureModels/\") %>%\n  select(path) %>%\n  mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%\n  unnest(cols = metrics)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nmodel_metrics_tbl %>%\n  mutate(\n    # Extract the model names\n    path = str_split(path, pattern = \"/\", simplify = T)[,3] %>% as_factor(),\n    auc  = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(fpr, tpr, color = path, linetype = auc)) +\n  geom_line(size = 1) +\n  \n  # just for demonstration purposes\n  geom_abline(color = \"red\", linetype = \"dotted\") +\n  \n  theme_new +\n  theme(\n    legend.direction = \"vertical\",\n  ) +\n  labs(\n    title = \"ROC Plot\",\n    subtitle = \"Performance of 3 Top Performing Models\"\n  )\n\n\n\n\n\n\n\n\n6 Precision vs Recall\n\nload_model_performance_metrics <- function(path, test_tbl) {\n  \n  model_h2o <- h2o.loadModel(path)\n  perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n  \n  perf_h2o %>%\n    h2o.metric() %>%\n    as_tibble() %>%\n    mutate(auc = h2o.auc(perf_h2o)) %>%\n    select(tpr, fpr, auc, precision, recall)\n  \n}\n\nmodel_metrics_tbl <- fs::dir_info(path = \"./../../assets/Models/PerformanceMeasureModels//\") %>%\n  select(path) %>%\n  mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%\n  unnest(cols = metrics)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nmodel_metrics_tbl %>%\n  mutate(\n    path = str_split(path, pattern = \"/\", simplify = T)[,3] %>% as_factor(),\n    auc  = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(recall, precision, color = path, linetype = auc)) +\n  geom_line(size = 1) +\n  theme_new + \n  theme(\n    legend.direction = \"vertical\",\n  ) +\n  labs(\n    title = \"Precision vs Recall Plot\",\n    subtitle = \"Performance of 3 Top Performing Models\"\n  )\n\n\n\n\n\n\n\n\n7 Gain & Lift\n\npredictions_tbl <- readRDS(\"./../../assets/DataSets/predictions.Rds\")\n\nranked_predictions_tbl <- predictions_tbl %>%\n  bind_cols(test_tbl) %>%\n  select(predict:Yes, went_on_backorder) %>%\n  # Sorting from highest to lowest class probability\n  arrange(desc(Yes))\n\n\nranked_predictions_tbl %>%\n  mutate(ntile = ntile(Yes, n = 10)) %>%\n  group_by(ntile) %>%\n  summarise(\n    cases = n(),\n    responses = sum(went_on_backorder == \"Yes\")\n  ) %>%\n  arrange(desc(ntile))\n\n\n\n  \n\n\ncalculated_gain_lift_tbl <- ranked_predictions_tbl %>%\n  mutate(ntile = ntile(Yes, n = 10)) %>%\n  group_by(ntile) %>%\n  summarise(\n    cases = n(),\n    responses = sum(went_on_backorder == \"Yes\")\n  ) %>%\n  arrange(desc(ntile)) %>%\n  \n  # Add group numbers (opposite of ntile)\n  mutate(group = row_number()) %>%\n  select(group, cases, responses) %>%\n  \n  # Calculations\n  mutate(\n    cumulative_responses = cumsum(responses),\n    pct_responses        = responses / sum(responses),\n    gain                 = cumsum(pct_responses),\n    cumulative_pct_cases = cumsum(cases) / sum(cases),\n    lift                 = gain / cumulative_pct_cases,\n    gain_baseline        = cumulative_pct_cases,\n    lift_baseline        = gain_baseline / cumulative_pct_cases\n  )\n\ncalculated_gain_lift_tbl \n\n\n\n  \n\n\ngain_lift_tbl <- performance_h2o %>%\n  h2o.gainsLift() %>%\n  as.tibble()\n\n## Gain Chart\n\ngain_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"lift\")) %>%\n  mutate(baseline = cumulative_data_fraction) %>%\n  rename(gain     = cumulative_capture_rate) %>%\n  # prepare the data for the plotting (for the color and group aesthetics)\n  pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\n\ngain_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Gain Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Gain\"\n  ) +\n  theme_new\n\n\n\n\n\n\n## Lift Plot\n\nlift_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"capture\")) %>%\n  mutate(baseline = 1) %>%\n  rename(lift = cumulative_lift) %>%\n  pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n\nlift_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Lift Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Lift\"\n  ) +\n  theme_new\n\n\n\n\n\n\n\n\n8 Dashboard with cowplot\n\nlibrary(cowplot)\nlibrary(glue)\n\n\n# set values to test the function while building it\nh2o_leaderboard <- automl_models_h2o@leaderboard\nnewdata <- test_tbl\norder_by <- \"auc\"\nmax_models <- 4\nsize <- 1\n\nplot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c(\"auc\", \"logloss\"),\n                                 max_models = 3, size = 1.5) {\n  \n  # Inputs\n  \n  leaderboard_tbl <- h2o_leaderboard %>%\n    as_tibble() %>%\n    slice(1:max_models)\n  \n  newdata_tbl <- newdata %>%\n    as_tibble()\n  \n  # Selecting the first, if nothing is provided\n  order_by      <- tolower(order_by[[1]]) \n  \n  # Convert string stored in a variable to column name (symbol)\n  order_by_expr <- rlang::sym(order_by)\n  \n  # Turn of the progress bars ( opposite h2o.show_progress())\n  h2o.no_progress()\n  \n  # 1. Model metrics\n  \n  get_model_performance_metrics <- function(model_id, test_tbl) {\n    \n    model_h2o <- h2o.getModel(model_id)\n    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))\n    \n    perf_h2o %>%\n      h2o.metric() %>%\n      as.tibble() %>%\n      select(threshold, tpr, fpr, precision, recall)\n    \n  }\n  \n  model_metrics_tbl <- leaderboard_tbl %>%\n    mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%\n    unnest(cols = metrics) %>%\n    mutate(\n      model_id = as_factor(model_id) %>% \n        # programmatically reorder factors depending on order_by\n        fct_reorder(!! order_by_expr, \n                    .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n      auc      = auc %>% \n        round(3) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id)),\n      logloss  = logloss %>% \n        round(4) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id))\n    )\n  \n  \n  # 1A. ROC Plot\n  \n  p1 <- model_metrics_tbl %>%\n    ggplot(aes(fpr, tpr, color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    theme_new +\n    labs(title = \"ROC\", x = \"FPR\", y = \"TPR\") +\n    theme(legend.direction = \"vertical\") \n  \n  \n  # 1B. Precision vs Recall\n  \n  p2 <- model_metrics_tbl %>%\n    ggplot(aes(recall, precision, color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    theme_new +\n    labs(title = \"Precision Vs Recall\", x = \"Recall\", y = \"Precision\") +\n    theme(legend.position = \"none\") \n  \n  \n  # 2. Gain / Lift\n  \n  get_gain_lift <- function(model_id, test_tbl) {\n    \n    model_h2o <- h2o.getModel(model_id)\n    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n    \n    perf_h2o %>%\n      h2o.gainsLift() %>%\n      as.tibble() %>%\n      select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)\n    \n  }\n  \n  gain_lift_tbl <- leaderboard_tbl %>%\n    mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%\n    unnest(cols = metrics) %>%\n    mutate(\n      model_id = as_factor(model_id) %>% \n        fct_reorder(!! order_by_expr, \n                    .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n      auc  = auc %>% \n        round(3) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id)),\n      logloss = logloss %>% \n        round(4) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id))\n    ) %>%\n    rename(\n      gain = cumulative_capture_rate,\n      lift = cumulative_lift\n    ) \n  \n  # 2A. Gain Plot\n  \n  p3 <- gain_lift_tbl %>%\n    ggplot(aes(cumulative_data_fraction, gain, \n               color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size,) +\n    geom_segment(x = 0, y = 0, xend = 1, yend = 1, \n                 color = \"red\", size = size, linetype = \"dotted\") +\n    theme_new +\n    expand_limits(x = c(0, 1), y = c(0, 1)) +\n    labs(title = \"Gain\",\n         x = \"Cumulative Data Fraction\", y = \"Gain\") +\n    theme(legend.position = \"none\")\n  \n  # 2B. Lift Plot\n  \n  p4 <- gain_lift_tbl %>%\n    ggplot(aes(cumulative_data_fraction, lift, \n               color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    geom_segment(x = 0, y = 1, xend = 1, yend = 1, \n                 color = \"red\", size = size, linetype = \"dotted\") +\n    theme_new +\n    expand_limits(x = c(0, 1), y = c(0, 1)) +\n    labs(title = \"Lift\",\n         x = \"Cumulative Data Fraction\", y = \"Lift\") +\n    theme(legend.position = \"none\") \n  \n  \n  # Combine using cowplot\n  \n  # cowplot::get_legend extracts a legend from a ggplot object\n  p_legend <- get_legend(p1)\n  # Remove legend from p1\n  p1 <- p1 + theme(legend.position = \"none\")\n  \n  # cowplot::plt_grid() combines multiple ggplots into a single cowplot object\n  p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2)\n  \n  # cowplot::ggdraw() sets up a drawing layer\n  p_title <- ggdraw() + \n    \n    # cowplot::draw_label() draws text on a ggdraw layer / ggplot object\n    draw_label(\"H2O Model Metrics\", size = 18, fontface = \"bold\", \n               color = \"#2C3E50\")\n  \n  p_subtitle <- ggdraw() + \n    draw_label(glue(\"Ordered by {toupper(order_by)}\"), size = 10,  \n               color = \"#2C3E50\")\n  \n  # Combine everything\n  ret <- plot_grid(p_title, p_subtitle, p, p_legend, \n                   \n                   # Adjust the relative spacing, so that the legends always fits\n                   ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))\n  \n  h2o.show_progress()\n  \n  return(ret)\n  \n}\n\nautoml_models_h2o@leaderboard %>%\n  plot_h2o_performance(newdata = test_tbl, order_by = \"logloss\", \n                       size = 0.5, max_models = 4)"
  },
  {
    "objectID": "content/01_journal/Regression.html",
    "href": "content/01_journal/Regression.html",
    "title": "Supervised ML - Regression",
    "section": "",
    "text": "1 Libraries\n\nlibrary(recipes)\n\n#> Loading required package: dplyr\n\n\n#> \n#> Attaching package: 'dplyr'\n\n\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n\n\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\n\n\n#> \n#> Attaching package: 'recipes'\n\n\n#> The following object is masked from 'package:stats':\n#> \n#>     step\n\nlibrary(parsnip)\nlibrary(yardstick)\nlibrary(tidymodels)\n\n#> ── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n\n\n#> ✔ broom        1.0.4     ✔ rsample      1.1.1\n#> ✔ dials        1.2.0     ✔ tibble       3.2.1\n#> ✔ ggplot2      3.4.2     ✔ tidyr        1.3.0\n#> ✔ infer        1.0.4     ✔ tune         1.1.1\n#> ✔ modeldata    1.1.0     ✔ workflows    1.1.3\n#> ✔ purrr        1.0.1     ✔ workflowsets 1.0.1\n\n\n#> ── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n#> ✖ purrr::discard() masks scales::discard()\n#> ✖ dplyr::filter()  masks stats::filter()\n#> ✖ dplyr::lag()     masks stats::lag()\n#> ✖ recipes::step()  masks stats::step()\n#> • Use tidymodels_prefer() to resolve common conflicts.\n\n\n\n2 Data set\n\nbike_orderlines_tbl <- readRDS(\"./../../assets/DataSets/bike_orderlines.rds\")\n\n\n3 Split the data into training and testing sets\n\nset.seed(123)  # For reproducibility\nbikes_split <- initial_split(bike_orderlines_tbl, prop = 0.8)\nbikes_train <- training(bikes_split)\nbikes_test <- testing(bikes_split)\n\n\n4 Create a model\n\nset.seed(1234)\nmodel_07_boost_tree_xgboost <- boost_tree(\n  mode = \"regression\",\n  mtry = 30,\n  learn_rate = 0.25,\n  tree_depth = 7\n) %>%\n  set_engine(\"xgboost\")\n\n\n5 Create a recipe\n\nbikes_recipe <- recipe(price ~ ., data = bikes_train) %>%\n  step_rm(order_id, order_line, order_date, url, location, lat, lng) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  prep()\n\n\n6 Create workflow\n\nbikes_wflow <- \n  workflow() %>% \n  add_model(model_07_boost_tree_xgboost) %>% \n  add_recipe(bikes_recipe)\n\nbikes_wflow\n\n#> ══ Workflow ════════════════════════════════════════════════════════════════════\n#> Preprocessor: Recipe\n#> Model: boost_tree()\n#> \n#> ── Preprocessor ────────────────────────────────────────────────────────────────\n#> 2 Recipe Steps\n#> \n#> • step_rm()\n#> • step_dummy()\n#> \n#> ── Model ───────────────────────────────────────────────────────────────────────\n#> Boosted Tree Model Specification (regression)\n#> \n#> Main Arguments:\n#>   mtry = 30\n#>   tree_depth = 7\n#>   learn_rate = 0.25\n#> \n#> Computational engine: xgboost\n\n\n\n7 Fit\n\nbikes_fit <- \n  bikes_wflow %>% \n  fit(data = bikes_train)\n\n\n8 Predict\n\npredictions <- predict(bikes_fit, bikes_test)\n\npredictions"
  }
]